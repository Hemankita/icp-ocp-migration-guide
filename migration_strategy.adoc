== Migration Strategy – ICP Cluster migration
:toc:
:toc-placement!:


toc::[]

=== OpenShift Installation

To migrate from ICP to Openshift, the recommended approach is a blue-green
deployment where an Openshift cluster is created alongside an existing ICP
cluster, the workload is migrated from ICP to Openshift, load balancers or DNS
entries are updated to point clients at the new cluster, and the ICP cluster is
retired.

We highly recommend infrastructure automation to create new clusters for
Openshift. This provides the quickest path for new cluster creation. We have
published infrastructure automation templates for ICP on various cloud
platforms, for example:

https://github.com/ibm-cloud-architecture/terraform-icp-vmware

Terraform Examples for OpenShift will be published soon.

In scenarios where resources are limited, the approach involves draining and
removing under-utilized ICP worker nodes and using the capacity to build a small
Openshift control plane with a few workers. Depending on available capacity, an
Openshift cluster can start as a single master and scale up to three masters as
capacity is made available.

=== User Migration

ifndef::env-github[]

include::./migration/ldap_migration.adoc[LDAP Migration]

include::./migration/rbac_migration.adoc[RBAC Migration]

endif::[]

ifdef::env-github[]

* link:./migration/ldap_migration.adoc[LDAP Migration]

* link:./migration/rbac_migration.adoc[RBAC Migration]

endif::[]


=== Workload Migration

In this section we used a case study of migrating our cloud native reference
application BlueCompute that was running on ICP to Openshift.

https://github.com/ibm-cloud-architecture/refarch-cloudnative-kubernetes

ifndef::env-github[]

include::./migration/container_mod.adoc[Modifying Containers to run in Openshift]

include::./migration/container_scc.adoc[Running Containers using SecurityContextConstraints in Openshift]

endif::[]


ifdef::env-github[]

* link:./migration/container_mod.adoc[Modifying Containers to run in Openshift]

* link:./migration/container_scc.adoc[Running Containers using SecurityContextConstraints in Openshift]

endif::[]

==== DevOps and Developer toolchains

Generally, ICP was not opinionated on DevOps, and if the toolchain is outside of
the platform there is no reason for this to change when migrating to Openshift.

Openshift’s value proposition does include some developer productivity tools
such as Source-to-Image (S2I) and containerized Jenkins, as well as tech preview
and support for Openshift Pipelines and CodeReady Workspaces, but in a migration
scenario where we are migrating existing workload from ICP these are not
required to change. We can simply treat Openshift as another Kubernetes platform
we are deploying to.

===== Jenkins server migration

One scenario that bears mentioning is if the CI toolchain was deployed to ICP,
typically using the community Helm chart. For example, In the BlueCompute case,
this was done using a containerized Jenkins instance. All of the stages in the
pipelines run as containers in Kubernetes using the Kubernetes plugin.

https://github.com/ibm-cloud-architecture/refarch-cloudnative-devops-kubernetes

The best practice around Jenkins is for the pipelines themselves to be written
using a declarative `+Jenkinsfile+` stored with the code. The pipeline logic is
then treated as code and stored with the application source code. In
BlueCompute, Jenkinsfiles are stored with each microservice, so we only needed
to create an instance of Jenkins in Openshift.

Openshift has both ephemeral and persistent Jenkins in the catalog which is very
comparable to the Jenkins Helm chart. This instance of Jenkins will
automatically install the Kubernetes and Openshift client plugins, so
Jenkinsfile that uses podTemplates that spin up containers to run stages in the
pipeline should ``just work''.

As pipeline stages are run in containers, there are security issues when a
particular stage attempts to run a container that requires root access, mounts a
hostpath, etc. Most runtime build images don’t run as root (e.g. maven, gradle,
etc).

One security problem when using Kubernetes plugin is how to build the container
image itself, which must run in a container. In Openshift this is further
complicated that the default `+restricted+` SCC disallows hostmounting the
docker socket, running as root, or running in privileged mode without changing
the SCC. The community is still attempting to resolve this problem, since
traditionally the Docker tool requires root and many of the functions involved
in building a container image requires various elevated privileges.

There are a few projects at various stages in development as of this writing
that can build container images without docker, which run fine outside of a
container, but none are perfect inside of a container.

* https://github.com/GoogleContainerTools/kaniko[kaniko]
* https://github.com/cyphar/orca-build[orca-build]
* https://github.com/genuinetools/img[img]
* https://github.com/uber/makisu[makisu]
* https://github.com/containers/buildah[buildah]

When using these tools, we can relax the security on them by adding the SCC to
the `+jenkins+` service account in the namespace where the pod runs. For
example, kaniko requires `+root+` access, so we can simply add the `+anyuid+`
scc to enable kaniko:

....
$ oc adm policy add-scc-to-user anyuid -z jenkins
....

Note the security implications of the above, that while containers are running
with elevated privileges, that any other workload running on the same worker
node may be vulnerable to attack. It may make sense to use a `+nodeSelector+` on
all projects where jenkins will run to isolate jenkins workloads to just a few
nodes:
https://docs.openshift.com/container-platform/3.11/admin_guide/managing_projects.html#setting-the-project-wide-node-selector

Alternatively, if we are running the Jenkins pipeline in Openshift, we can
leverage the Openshift BuildConfig to build the container image, which provides
a controlled environment for producing the container image without exposing
`+root+` access on any worker nodes. By providing just a `+Dockerfile+` and a
context, Openshift will build and push the resulting image to the Openshift
private registry and track it using an ImageStream, and we can then provide an
additional stage using https://github.com/containers/skopeo[skopeo] to push this
to an external registry.

We have posted an example pipeline we used at a customer that leverages this at
the following git repository:

https://github.com/jkwong888/liberty-hello-world-openshift/blob/master/Jenkinsfile

===== Declarative deployment and GitOps adoption

==== Using a Backup/Restore approach using Velero

==== Converting PodSecurityPolicy to SecurityContextConstraints

==== Helm Charts

==== Application LoadBalancer cutover

=== Storage Migration

ifndef::env-github[]
:leveloffset: +1

include::./migration/storage.adoc[Storage Migration]

:leveloffset:0 

endif::[]

ifdef::env-github[]

* link:./migration/storage.adoc[Storage Migration]

endif::[]



=== Platform Data Migration

==== Monitoring Data

==== Historical Log Data
