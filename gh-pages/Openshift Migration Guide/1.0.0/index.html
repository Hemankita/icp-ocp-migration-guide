<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>ICP to OpenShift migration guide :: Openshift Migration Guide</title>
    <link rel="canonical" href="https://github.ibm.com/CASE/openshift-migration-guide/Openshift%20Migration%20Guide/1.0.0/index.html">
    <meta name="generator" content="Antora 2.1.2">
    <link rel="stylesheet" href="../../_/css/site.css">
  </head>
  <body class="article">
<header class="header" role="banner">
  <nav class="navbar">
    <div class="navbar-brand">
      <div class="navbar-item">
        <p>Openshift Migration Guide</p>
      </div>
    </div>
    <div id="topbar-nav" class="navbar-menu">
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="Openshift Migration Guide" data-version="1.0.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Openshift Migration Guide</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Migration</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="migration/container_mod.html">Container Modification</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="migration/container_scc.html">Security Context Constraints</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="migration/ldap_migration.html">User Authentication Migration (LDAP)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="migration/rbac_migration.html">User Authorization Migration (RBAC)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="migration/storage.html">Storage Migration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="migration/app_resource_migration.html">App resource Migration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="migration/cd_migration.html">GitOps Migration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="migration/jenkins_migration.html">Jenkins server Migration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Openshift Migration Guide</span>
    <span class="version">1.0.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <span class="title">Openshift Migration Guide</span>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="index.html">1.0.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main>
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="index.html" class="home-link is-current"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Openshift Migration Guide</a></li>
    <li><a href="index.html">ICP to OpenShift migration guide</a></li>
  </ul>
</nav>
</div>
<article class="doc">
<h1 class="page">ICP to OpenShift migration guide</h1>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel2">
<li><a href="#_key_differences_between_icp_and_openshift">Key differences between ICP and OpenShift</a></li>
<li><a href="#_development_experience">Development Experience</a></li>
<li><a href="#_devops">DevOps</a></li>
<li><a href="#_infrastructure">Infrastructure</a>
<ul class="sectlevel2">
<li><a href="#_hardware_and_hypervisor">Hardware and hypervisor</a></li>
<li><a href="#_iaas">IaaS</a></li>
<li><a href="#_operating_system">Operating System</a></li>
</ul>
</li>
<li><a href="#_storage">Storage</a></li>
<li><a href="#_security">Security</a>
<ul class="sectlevel2">
<li><a href="#_selinux">SELinux</a></li>
<li><a href="#_podsecuritypolicy_vs_securitycontextconstraints">PodSecurityPolicy vs SecurityContextConstraints</a></li>
<li><a href="#_identity_providers">Identity Providers</a></li>
<li><a href="#_role_based_access_control">Role-based Access Control</a></li>
</ul>
</li>
<li><a href="#_networking">Networking</a>
<ul class="sectlevel2">
<li><a href="#_openshift_sdn">OpenShift SDN</a></li>
<li><a href="#_openshift_sdn_architecture">OpenShift SDN Architecture</a></li>
<li><a href="#_ip_address_management">IP Address Management</a></li>
<li><a href="#_pod_routing_and_route_propagation">Pod Routing and Route Propagation</a></li>
<li><a href="#_network_isolation">Network Isolation</a></li>
<li><a href="#_networkpolicy">NetworkPolicy</a></li>
<li><a href="#_egressnetworkpolicy_and_egressrouter">EgressNetworkPolicy and EgressRouter</a></li>
<li><a href="#_dns">DNS</a></li>
<li><a href="#_routes_vs_ingress">Routes vs Ingress</a></li>
<li><a href="#_external_integration_with_f5_load_balancer">External Integration with F5 Load Balancer</a></li>
</ul>
</li>
<li><a href="#_operation_cluster_management_monitoring_and_logging">Operation – Cluster Management, Monitoring and Logging</a>
<ul class="sectlevel2">
<li><a href="#_cluster_management">Cluster Management</a></li>
<li><a href="#_monitoring">Monitoring</a></li>
<li><a href="#_logging">Logging</a></li>
</ul>
</li>
<li><a href="#_migration_strategy_icp_cluster_migration">Migration Strategy – ICP Cluster migration</a>
<ul class="sectlevel2">
<li><a href="#_openshift_installation">OpenShift Installation</a></li>
<li><a href="#_user_migration">User Migration</a></li>
<li><a href="#_workload_migration">Workload Migration</a></li>
<li><a href="#_devops_and_developer_toolchains">DevOps and Developer toolchains</a></li>
<li><a href="#_storage_migration">Storage Migration</a></li>
<li><a href="#_platform_data_migration">Platform Data Migration</a></li>
</ul>
</li>
</ul>
</div>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Note that the guide is still being developed and updates are still ongoing.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_key_differences_between_icp_and_openshift"><a class="anchor" href="#_key_differences_between_icp_and_openshift"></a>Key differences between ICP and OpenShift</h3>
<div class="paragraph">
<p>IBM Cloud Private (ICP) and Red Hat OpenShift are different Kubernetes
distributions. Fundamentally, both are based on the core Kubernetes
technologies. Thus, they bring relatively consistent experience for application
development and platform operation. From migration perspective, we’ll focus on
the key differences between the two platforms.</p>
</div>
<!-- toc disabled -->
<div class="paragraph">
<p>The following is the summary of the key differences:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top"><strong>ICP</strong></th>
<th class="tableblock halign-left valign-top"><strong>OpenShift</strong></th>
<th class="tableblock halign-left valign-top"><strong>Migration Effort</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Infrastructure</strong></p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hardware</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">x86_64Power (ppc64le)IBM Z</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">X86_64Power</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Z is large migration effort,
OpenShift generally runs wherever RHEL runs</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Operating System</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Red Hat Enterprise Linux 7.3, 7.4, 7.5Ubuntu 18.04 LTS and
16.04 LTSSUSE Linux Enterprise (SLES) 12</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">RHELRed Hat CoreOS (RHCOS)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Migrating
from Ubuntu / Suse to OpenShift, customer may require operation procedure change
such as OS patching, security certification etc.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">IaaS provider</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">VMWare/OpenStackMost public cloud IaaS providers</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">VMWare/OpenStackMost of the public cloud IaaS providerIBM Cloud and Azure
provide managed OpenShift service</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Need to pay attention on the networking and
storage for the specific IaaS provider, as well as any automation code specific
to an IaaS provider</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">HA Cluster Topology</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">MasterProxyManagementVA</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">MasterWorker</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Migration effort
should be part of the OpenShift planning and installation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Installation</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ansible installer Delivered as Docker image</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ansible installer
(v4.x completely changed the installation procedure with Operator) Delivered as
RPM or Docker image</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Installation procedure particularly any automation script
requires significant change</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container Registry</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ICP private registryExternal Docker Registry</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift
Container Registry (OCR)External Docker Registry</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small effort</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kubernetes Version</p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.11</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kubernetes is generally stable after 1.9 but there
are some features (e.g. storage-related) that are beta in 1.11</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Development</strong></p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Local dev environment</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Vagrant based local ICP cluster with ICP Community
edition</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">MinishiftAll-in-one OKD</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small effort</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Development Layer</p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Projects</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Provides higher level Kubernetes construct
simplifying deployments.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Development tools</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Standard KubernetesMicroprofile</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Standard
KubernetesSource-to-image (S2I)fabric8</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Not too much needs to done for
developers</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DevOps</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Platform neutral DevOps toolchain</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Packaged Jenkins for CIImageStream
for container images2i</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Large effort.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Application package</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Standard Kubernetes yamlHelmOperator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Standard Kubernetes
yamlOpenShift templateOpenShift ApplicationOperator</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Medium to large effort</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deployment</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Standard Kubernetes deployment and strategy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift opionated
DeploymentConfig and ImageStream</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Suggest to keep the Kubernetes standard
approach. Small effort</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Operation</strong></p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Command Line Tool</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">kubectl</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">oc (superset of kubetl)kubectl</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small effort when
cli is used any operation or devops automation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">User interface</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ICP UI</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift UI</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small effort</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multi-tenancy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Standard Kubernetes namespace and RBAC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift Project with
RBACOperator can apply Quotas and limit per project or cluster</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Medium effort</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Networking</strong></p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SDN (Software Defined Network)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Default on CalicoSupport other Kubernetes
supported SDN</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Default to Red Hat Open vSwitch SDNCan use other Kubernetes
supported SDN as well</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small effort. Most of these are internal to ICP and
OpenShift</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cluster DNS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">CoreDNS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">SkyDNS (3.x)CoreDNS (4.x)</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">External Access for Services</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ingress ControllerLoad Balancer</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Router
(HAProxy)Load Balancer</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Medium effort to update the service exposure</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Storage</strong></p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">File Storage</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GlusterFSNFS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">GlusterFSNFSCeph</p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Security</strong></p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container permission</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Allows to run container as root</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Forbids to run a
container as root by default (best practice)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small effort to rebuild the
application container</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Authentication</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenID with primarily LDAP identify provider</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">OAuth with
identity providerOpenShift supports different kinds of IAM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small effort</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Authorization</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">RBAC as aboveKubernetes called Pod Security Policies (PSP) beta</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">RBAC as aboveOpenShift Security Context Constraint (SCC)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small effort, some
changes needed in particular to address the SCC</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Securing the master</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TLS to masterX.509 certifate or token to access API server</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">TLSX.509 certifate or token to access API serverProject quota to limit the
token rate</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small effort</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>For further reading, you can check this blog published earlier
<a href="https://apps.na.collabserv.com/blogs/ca5e7833-78b8-481c-8a14-ba70b22a20ce/entry/Comparing_IBM_Cloud_Private_ICP_with_RedHat_OpenShift?lang=en_us" class="bare">https://apps.na.collabserv.com/blogs/ca5e7833-78b8-481c-8a14-ba70b22a20ce/entry/Comparing_IBM_Cloud_Private_ICP_with_RedHat_OpenShift?lang=en_us</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_development_experience"><a class="anchor" href="#_development_experience"></a>Development Experience</h3>
<div class="sect3">
<h4 id="_openshift_development_environment"><a class="anchor" href="#_openshift_development_environment"></a>OpenShift Development Environment</h4>
<div class="paragraph">
<p>The goal of OpenShift is to provide a great experience for both Developers and
System Administrators to develop, deploy, and run containerized applications.
Developers should love using OpenShift because it enables them to take advantage
of both containerized applications and orchestration without having to know the
details. Developers are free to focus on their code instead of spending time
writing Dockerfiles and running docker builds.</p>
</div>
<div class="paragraph">
<p>OpenShift is a full platform that incorporates several upstream projects while
also providing additional features and functionality to make those upstream
projects easier to consume. The core of the platform is containers and
orchestration. For the container side of the house, the platform uses images
based upon the docker image format. For the orchestration side, it is based on
upstream Kubernetes project. Beyond these two upstream projects, there are a set
of additional Kubernetes objects such as routes and deployment configs.</p>
</div>
</div>
<div class="sect3">
<h4 id="_standard_interfaces_differences_oc_tool_usage_vs_kubectl_and_helm"><a class="anchor" href="#_standard_interfaces_differences_oc_tool_usage_vs_kubectl_and_helm"></a>Standard Interfaces Differences (oc tool usage vs. kubectl and HELM)</h4>
<div class="paragraph">
<p>Both Developers and Operators communicate with the OpenShift Platform via one of
the following methods:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Command Line Interface:</strong> <em>The command line tool that we will be using as part
of this training is called the <strong>oc </strong>tool.</em> This tool is written in the Go
programming language and is a single executable that is provided for Windows, OS
X, and the Linux Operating Systems.</p>
</li>
<li>
<p><strong>A Web Console:</strong> User friendly graphical interface</p>
</li>
<li>
<p><strong>REST API:</strong> Both the command line tool and the web console actually
communicate to OpenShift via the same method, the REST API. Having a robust API
allows users to create their own scripts and automation depending on their
specific requirements. For detailed information about the REST API, check out
the official documentation
at: https://docs.openshift.org/latest/rest_api/index.html[<a href="https://docs.OpenShift.org/latest/rest_api/index.html" class="bare">https://docs.OpenShift.org/latest/rest_api/index.html</a>]</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>IBM Cloud Private also provides a CLI. Many interactions with ICP though happen
through the standard Kubernetes CLI called <strong>kubectl.</strong> Developers also made use
of <strong>HELM</strong> as a package manager to deploy workloads. Whereas the pattern for ICP
developers was to make heavy use of kubectl or HELM to deploy workloads and
applications, OpenShift users often make more use of the <strong>oc</strong> commandline tool
than kubectl. (<em>Note: HELM can be used in OpenShift environment but it must be
installed into OpenShift. IBM Cloud Paks provide this ability as a core service
over OpenShift</em>).</p>
</div>
<div class="paragraph">
<p>OpenShift aims to greatly simplify development and deployment of applications,
thus providing a layer over Containers (much like a Cloud Foundry would), and
the <strong>oc tool</strong> provides those tools.</p>
</div>
</div>
<div class="sect3">
<h4 id="_projects"><a class="anchor" href="#_projects"></a>Projects</h4>
<div class="paragraph">
<p>OpenShift is often referred to as a container application platform in that it is
a platform designed for <strong><em>the development and deployment of containers.</em></strong></p>
</div>
<div class="paragraph">
<p>To contain your application, OpenShift use <strong>projects</strong>. The reason for having a
project to contain your application is to allow for controlled access and quotas
for developers or teams. More technically, it&#8217;s a visualization of the
Kubernetes namespace based on the developer access controls. Under the hood,
while <code>project'' is a separate object returned by the OpenShift API, there is a
one-to-one mapping between </code>projects'' and ``namespaces'' in Kubernetes.</p>
</div>
<div class="paragraph">
<p>The typical experience goes something like:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Developer logs in to the console or CLI and creates a project</p>
</li>
<li>
<p>Add artifacts to project. This can take several forms, for example</p>
<div class="ulist">
<ul>
<li>
<p>Deploy an existing Image (usually Docker based) and with optionally
additional YAML files.</p>
</li>
<li>
<p>Create an application out of templates.</p>
</li>
<li>
<p>Create pipelines out of several approaches. (OpenShift has a built in
mechanism called Source 2 Image, of s2i that can deploy straight from a git
repository)</p>
</li>
</ul>
</div>
</li>
<li>
<p>Configure resources.</p>
<div class="ulist">
<ul>
<li>
<p>Items include exposing a Route (Described later in the article)</p>
</li>
<li>
<p>Scale Pods.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>When you create a Project and add a deployment, several of the Kubernetes
Objects are created for you by default. This includes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Pods:</strong> Where your containers run which you can begin to scale immediately.</p>
</li>
<li>
<p><strong>Services:</strong> provide internal abstraction and load balancing within an
OpenShift environment, but sometimes clients (users, systems, devices,
etc.) <strong>outside</strong> of OpenShift need to access an application.</p>
</li>
<li>
<p><strong>Routes:</strong> The way that external clients are able to access applications
running in OpenShift. (Similar to Ingress or Node Ports).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A great way to get started with the development experience is through the
following website. <a href="https://learn.openshift.com/">https://learn.OpenShift.com/</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_migration_of_applications_from_icp_to_openshift"><a class="anchor" href="#_migration_of_applications_from_icp_to_openshift"></a>Migration of applications from ICP to OpenShift.</h4>
<div class="paragraph">
<p>There are actually many paths you can take to do this.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Install HELM either through open source or through IBM Cloud Paks. An example
of this is here
(<a href="https://github.com/ibm-cloud-architecture/refarch-cloudnative-kubernetes/tree/spring#deploy-bluecompute-to-an-openshift-cluster">https://github.com/ibm-cloud-architecture/refarch-cloudnative-kubernetes/tree/spring#deploy-bluecompute-to-an-OpenShift-cluster</a>)</p>
</li>
<li>
<p>Take existing Docker Images and applications, update YAML, and create a
project with the oc tool. You can then use one of the mechanisms described
earlier. This will require you to update existing CI/CD pipleines but moves you
closer to the OpenShift environment.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_development_environments"><a class="anchor" href="#_development_environments"></a>Development Environments</h4>
<div class="paragraph">
<p>OpenShift developers can use several approaches to local development.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Develop code and Docker images locally and deploy to a remote cluster. There
are several ``managed OpenShift Options'' on various public clouds.</p>
</li>
<li>
<p>If you need to run a local kubrnetes distribution you can use.</p>
<div class="ulist">
<ul>
<li>
<p><strong>Minikube:</strong> This is the standard community Kubernetes. However, this will
require you maintain duplicate YAML artifacts. This approach is not recommended.</p>
</li>
<li>
<p><strong>OKD:</strong> This is the Origin Community Distribution that powers OpenShift. You
can access it here: <a href="https://www.okd.io/" class="bare">https://www.okd.io/</a><span class="[.underline"><a href="https://www.okd.io/" class="bare">https://www.okd.io/</a></span>]. OKD
provides a feature complete version of OpenShift.</p>
</li>
<li>
<p><strong>Minishift</strong> is a tool that helps you run OKD locally by launching a
single-node OKD cluster inside a virtual machine. With Minishift you can try out
OKD or develop with it, day-to-day, on your local machine. You can run Minishift
on the Windows, macOS, and GNU/Linux operating systems. More information can be
found here: <a href="https://www.okd.io/minishift/" class="bare">https://www.okd.io/minishift/</a></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>OpenShift is not opinionated on the application stack and provides templates for
various popular OpenSource frameworks such as Spring, Java EE, JBoss, Quarkus,
Node, etc…. A great place to learn about various types of applications you can
build is here:
<a href="https://learn.openshift.com/middleware/">https://learn.OpenShift.com/middleware/</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_additional_tools_clis_and_frameworks"><a class="anchor" href="#_additional_tools_clis_and_frameworks"></a>Additional tools, CLI’s, and Frameworks</h4>
<div class="paragraph">
<p>In addition to the oc tool, there are several more CLI’s, tools, and frameworks
that you should be aware of.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>odo:</strong> a CLI tool for developers who are writing, building, and deploying
applications on OpenShift. With odo, developers get an opinionated CLI tool that
supports fast, iterative development. odo abstracts away Kubernetes and
OpenShift concepts so developers can focus on what&#8217;s most important to them:
code. odo was created to improve the developer experience with OpenShift.
Existing tools such as oc are more operations-focused and require a deep
understanding of Kubernetes and OpenShift concepts. More information can be
found here: <a href="https://openshiftdo.org/">https://OpenShiftdo.org/</a></p>
</li>
<li>
<p><strong>Source-to-Image (S2I):</strong> Source-to-Image (S2I) is a toolkit and workflow for
building reproducible container images from source code. It is worth noting that
you can use any CI / CD tool with OpenShift as well. More information can be
found here:
<a href="https://github.com/openshift/source-to-image">https://github.com/OpenShift/source-to-image</a>.
We will discuss this more in the next section.</p>
</li>
<li>
<p><strong>CodeReady:</strong> Built on the open Eclipse Che project, Red Hat CodeReady
Workspaces provides developer workspaces, which include all the tools and the
dependencies that are needed to code, build, test, run, and debug applications.
More information can be found here:
<a href="https://developers.redhat.com/products/codeready-workspaces/overview" class="bare">https://developers.redhat.com/products/codeready-workspaces/overview</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>OpenShift developers can also use popular projects such as ISTIO, kNative, and
others on the platform</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>ISTIO</strong> is a service mesh that provides features such as routing, secure
communication, Circuit Breaker, and Application diagnostic tools. Istio is
supported throught he OpenShift Service Mesh offering, which is a Tech Preview
and will be GA at the end of Aug 2019. To learn how to use ISTIO on OpenShift,
go here:
<a href="https://learn.openshift.com/servicemesh/">https://learn.OpenShift.com/servicemesh/</a></p>
</li>
<li>
<p><strong>Knative</strong> extends Kubernetes to provide components for building, deploying,
and managing serverless applications</p>
</li>
<li>
<p><strong>Tekton</strong> is a cloud-native CI/CD framework where pipeline stages are executed
in containers. Tekton is part of the OpenShift Pipelines offering. For more
information go here:
<a href="https://blog.openshift.com/cloud-native-ci-cd-with-openshift-pipelines/">https://blog.OpenShift.com/cloud-native-ci-cd-with-OpenShift-pipelines/</a></p>
</li>
<li>
<p><strong>Operators</strong> are a framework for building Kubernetes-native applications. Red
Hat provides and SDK for getting up and running on creating Operators from Helm
charts, Ansible playbooks, and go code. For more information see:
<a href="https://github.com/operator-framework/getting-started" class="bare">https://github.com/operator-framework/getting-started</a></p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_ibm_cloud_pak_for_applications_and_additional_open_source_projects"><a class="anchor" href="#_ibm_cloud_pak_for_applications_and_additional_open_source_projects"></a>IBM Cloud Pak for Applications and additional Open Source projects</h4>
<div class="paragraph">
<p>IBM announced the <a href="https://www.ibm.com/cloud/cloud-pak-for-applications">Cloud Pak
for Applications</a> which includes support for IBM application runtimes such as
IBM WebSphere Liberty and middleware such as IBM MobileFirst Foundation</p>
</div>
<div class="paragraph">
<p>It also includes various recently-announced open source projects maintained by
IBM around developer tooling. These include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>[<strong>Kabanero</strong>]: <a href="https://kabanero.io" class="bare">https://kabanero.io</a>, which consists of CodeWind
<a href="https://codewind.dev" class="bare">https://codewind.dev</a> for IDE extensions to developer tools like Eclipse and
VSCode, and Appsody <a href="https://appsody.dev" class="bare">https://appsody.dev</a> for building templates for popular
runtimes</p>
</li>
<li>
<p><strong>Razee</strong> <a href="https://razee.io" class="bare">https://razee.io</a> for Continuous Deployment</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The IBM Cloud Pak for Applications is still in development and may include more
components in the future.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_devops"><a class="anchor" href="#_devops"></a>DevOps</h3>
<div class="paragraph">
<p>As mentioned earlier, OpenShift provides an opinionated development platform
around source-to-image (S2I) as a differentiator over upstream community
Kubernetes. As a comparison to ICP, it was not opinionated on DevOps beyond
providing (outdated) community Helm Charts for Jenkins. S2I is an integrated
build and deployment framework that developers can use to run code in containers
in the platform without additional infrastructure.</p>
</div>
<div class="paragraph">
<p>Note that if DevOps procedures are already mature and not tied to the platform,
and infrastructure is outside of the platform, it’s possible to reuse most of it
as OpenShift conforms to Kubernetes. There are some minor differences around
security which are discussed later in this document.</p>
</div>
<div class="paragraph">
<p>That said, a large part of OpenShift value proposition is that it’s an
integrated development platform in addition to being a container orchestrator.
OpenShift includes some CustomResourceDefinitions (CRDs) around continuous
integration (CI) and continuous deployment (CD) that enhance developer
productivity. As the controllers for these objects are built-in to the OpenShift
API, they are not portable outside of OpenShift.</p>
</div>
<div class="sect3">
<h4 id="_imagestream"><a class="anchor" href="#_imagestream"></a>ImageStream</h4>
<div class="paragraph">
<p>An ImageStream represents an image either in the internal OpenShift container
image registry, or in an external registry. An image in an external registry can
be mirrored and cached in the local container image registry.</p>
</div>
<div class="paragraph">
<p>There are a few related resources to ImageStreams:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The ImageStream resource represents the repository part of the image</p>
</li>
<li>
<p>The ImageStreamTag resource represents an individual tag, which points at the
hash of the image as stored in the registry. This hash is immutable and every
push to the tag will update the hash, assuming the image has changes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example, if we were to import docker.io/ibmcom/websphere-liberty:latest, the
ImageStream part would be <code>docker.io/ibmcom/websphere-liberty'', and the tag
would be </code>latest''. The ImageStreamTag would represent the pointer to the image
represented by ``docker.io/ibmcom/websphere-liberty:latest'', which changes
every time someone pushes to the ibmcom/websphere-liberty:latest tag.</p>
</div>
<div class="paragraph">
<p>OpenShift will deploy the image hash in deployments and the ImageStreamTag
tracks the upstream images as they change. As such, we can use ImageStreams to
track changes to images even if the image in the original tag changes.</p>
</div>
<div class="paragraph">
<p>Images in external registries can be imported into OpenShift as ImageStreams,
and mirrored on a schedule. ImageStream changes can trigger builds or
redeployments; this can be useful in cases such as triggering rebuilds on a
nightly patched image updates for base images, or as part of a continuous
deployment procedure where image tags are used to track image deployments to
certain environments.</p>
</div>
<div class="paragraph">
<p>Additionally, since the ImageStream objects are stored in OpenShift/Kubernetes,
RBAC can be applied to them and they can be scoped to individual projects or
shared to multiple projects. This is similar to how ICP manages RBAC around
images as well in its private registry.</p>
</div>
<div class="paragraph">
<p>View the FAQ on the ImageStream here:
<a href="https://blog.openshift.com/image-streams-faq/">https://blog.OpenShift.com/image-streams-faq/</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_buildconfig"><a class="anchor" href="#_buildconfig"></a>BuildConfig</h4>
<div class="paragraph">
<p>For Continuous Integration, the BuildConfig is a CustomResource is used to
produce a target image based on inputs and triggers. The BuildConfig takes as
input:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Source code (such as a git repository) or binaries, (for example, a directory
as part of an external pipeline)</p>
</li>
<li>
<p>Source ImageStream (for example a base image like ibmcom/websphere-liberty)</p>
</li>
<li>
<p>Target ImageStream which contains the built application artifact</p>
<div class="paragraph">
<p>There are various strategies around BuildConfig, which control how the target
image stream is assembled:</p>
</div>
</li>
<li>
<p>Source strategy: this is the core of S2I where a builder image is provided
that builds the source and packages it into a target container image, then
pushes it into the OpenShift private registry. This requires the builder image
to have knowledge about how to turn code into a container image. For example,
for Java code, the builder image may run ``mvn package'', take the output
binaries and build an image from a Java runtime. Red Hat ships several builder
images for popular runtimes, but any custom runtimes or deviations from the
happy path may require additional work to support. Red Hat provides an
SDK/documentation on how to build custom builder images here:
<a href="https://github.com/openshift/source-to-image">https://github.com/OpenShift/source-to-image</a></p>
</li>
<li>
<p>Docker strategy: this is equivalent to running <code>docker build'' on a local
machine, except it is done through OpenShift. As part of this, the context
directory and a Dockerfile are uploaded to OpenShift where it the container
image is assembled from binaries. There are advantages to this, mainly that in
some CI scenarios in multi-tenant environments where the administrators do not
want to expose docker socket for direct </code>docker build'', as this exposes root
access on the machine where the container is assembled.</p>
</li>
<li>
<p>Pipeline strategy: this is equivalent to creating a staged build pipeline
through Jenkins. In this BuildConfig type, an embedded Jenkins declarative
pipeline is defined in the body of the resource. OpenShift will provision an
instance of Jenkins in the project to execute the build and will sync the build
status from Jenkins to the Build object (more on it below). The OpenShift
Application console contains some UI elements that show the build status from
Jenkins.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>An instance of an execution of BuildConfig is a Build. Builds can be triggered
when the upstream source is changed, when the source ImageStream changes, or
manually using "oc new-build". An execution of BuildConfig results in a new
Build object being created, which has a build number that increments every time
the build is run. BuildConfig can maintain build history for both successful and
unsuccessful builds. The build itself is run in a build pod.</p>
</div>
<div class="paragraph">
<p>For more information, see here:
<a href="https://docs.openshift.com/container-platform/3.11/dev_guide/builds/index.html">https://docs.OpenShift.com/container-platform/3.11/dev_guide/builds/index.html</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_deploymentconfig"><a class="anchor" href="#_deploymentconfig"></a>DeploymentConfig</h4>
<div class="paragraph">
<p>OpenShift has DeploymentConfigs, which is a precursor to the Kubernetes
Deployments. The DeploymentConfig resource is not portable to non-OpenShift
Kubernetes distributions. Note that OpenShift also supports the familiar
Deployment resource as well, so in terms of moving from ICP or other Kubernetes
distributions, offers basically zero migration effort and is more
community-friendly.</p>
</div>
<div class="paragraph">
<p>DeploymentConfig does provide deeper integration with ImageStreams, in that when
an ImageStream is updated, OpenShift can perform an update of the Deployment.
OpenShift can also extend this integration with ImageStreams to regular
Deployments by configuration, see
<a href="https://docs.openshift.com/container-platform/3.11/dev_guide/managing_images.html#using-is-with-k8s">https://docs.OpenShift.com/container-platform/3.11/dev_guide/managing_images.html#using-is-with-k8s</a>.</p>
</div>
<div class="paragraph">
<p>Additionally, DeploymentConfig supports a few advanced deployment strategies,
which are detailed here:
<a href="https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html">https://docs.OpenShift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html</a>.
Most notably, they claim support for ``canary'' deployments, although the
documentation suggests the regular rolling update is a form of canary deployment
(which it isn’t, as the deployment continues to get rolled over as soon as the
health checks pass). There is also support for A/B testing and blue-green
deployments.</p>
</div>
<div class="paragraph">
<p>There are additional features and differences between Deployments and
DeploymentConfigs in OpenShift. When a DeploymentConfig rolls out a deployment,
a <code>deploy'' pod is created that performs the actual deployment, as opposed to a
controller running on the master performing the rollout. This may be slightly
more scalable in very large clusters where many rolling deployments are
happening simultaneously. Additionally, rollouts may be paused and resumed as
needed. Also, a handy command is the </code>oc rollout latest'', which just
re-deploys the same version of the pod; this is useful if a ConfigMap has
changed and the pods need to restart to refresh them.</p>
</div>
<div class="paragraph">
<p>For more information, see here:
<a href="https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/how_deployments_work.html">https://docs.OpenShift.com/container-platform/3.11/dev_guide/deployments/how_deployments_work.html</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_templates"><a class="anchor" href="#_templates"></a>Templates</h4>
<div class="paragraph">
<p>OpenShift provides support for Template resources, which are regular OpenShift
objects with parametrized fields in them. This is similar to Helm template, but
without the advanced ability to generate random data, conditionals, or complex
variable types.</p>
</div>
<div class="paragraph">
<p>The ``oc process'' command is used to convert a template to a regular resource.
The Template is a list of one or more templated resources, and can be stored in
the OpenShift API for re-use, or processed from local filesystem. Templates form
the base for the "oc new-app" command which generates a list of resources from a
list of parameters.</p>
</div>
<div class="paragraph">
<p>Again, as templates are very OpenShift specific, use discretion before using.
There are several other open-source Kubernetes templating projects, for example
Helm and Kustomize, that are more portable and more community-friendly.
Generally Red Hat frowns upon Helm 2.x as server side tiller requires large
permissions and the helm client requires read access to the namespace where
tiller runs; Helm 3 addresses this by including tiller on client side.</p>
</div>
<div class="paragraph">
<p>See here for more information:
<a href="https://docs.openshift.com/container-platform/3.11/dev_guide/templates.html">https://docs.OpenShift.com/container-platform/3.11/dev_guide/templates.html</a></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_infrastructure"><a class="anchor" href="#_infrastructure"></a>Infrastructure</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This chapter explores the infrastructure consideration when migrating from ICP
to OpenShift. It covers the hardware platform, IaaS and hypervisors, operating
system and platform automation.</p>
</div>
<div class="sect2">
<h3 id="_hardware_and_hypervisor"><a class="anchor" href="#_hardware_and_hypervisor"></a>Hardware and hypervisor</h3>
<div class="paragraph">
<p>ICP can be deployed on (Linux) x86_64, Power (ppc64le) and IBM Z and LinuxOne.
OpenShift now can run x86_64 and Power hardware. Each has its own sizing
recommendation in terms of CPU, memory and disk space. You can reference the
system requirement for both below:</p>
</div>
<div class="paragraph">
<p>ICP (3.2) hardware requirement guide -
<a href="https://www.ibm.com/support/knowledgecenter/SSBS6K_3.2.0/supported_system_config/hardware_reqs.html" class="bare">https://www.ibm.com/support/knowledgecenter/SSBS6K_3.2.0/supported_system_config/hardware_reqs.html</a></p>
</div>
<div class="paragraph">
<p>OpenShift (3.11) hardware requirement -
<a href="https://docs.openshift.com/container-platform/3.11/install/prerequisites.html#hardware">https://docs.OpenShift.com/container-platform/3.11/install/prerequisites.html#hardware</a></p>
</div>
<div class="paragraph">
<p>Both ICP and OpenShift can run on Hypervisors like VMware, OpenStack and Hyper-V
in a private cloud environment. ICP is also supported on IBM PowerVC.</p>
</div>
</div>
<div class="sect2">
<h3 id="_iaas"><a class="anchor" href="#_iaas"></a>IaaS</h3>
<div class="paragraph">
<p>Both ICP and OpenShift can run on public or private IaaS. In public. We have
tested ICP on IBM Cloud, Azure, AWS, GCP, and Huawei Cloud. On the other hand,
we have tested OpenShift on IBM Cloud, Azure, AWS.</p>
</div>
<div class="paragraph">
<p>For OpenShift on public cloud, there are potentially 3 offering:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Managed OpenShift cluster. This includes IBM IKS managed OpenShift (beta) and
Azure Managed OpenShift</p>
</li>
<li>
<p>Guided-provision OpenShift cluster. The IaaS vendors provide guided automation
procedure to provision a full OpenShift cluster either through UI or automation
scripts. For example, Azure OpenShift cluster and AWS OpenShift quickstart.</p>
</li>
<li>
<p>Build your own cluster. End user provisions IaaS VMs (or bare metal), then
install OpenShift on top of the VMs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>ICP doesn’t have a managed edition.</p>
</div>
</div>
<div class="sect2">
<h3 id="_operating_system"><a class="anchor" href="#_operating_system"></a>Operating System</h3>
<div class="paragraph">
<p>This is where you should pay the most attention when migrating from ICP.</p>
</div>
<div class="paragraph">
<p>Both platforms can only run on top of Linux OS. ICP supports Red Hat Enterprise
Linux (RHEL) 7.3, 7.4 and 7.5, Ubuntu 18.04 LTS and 16.04 LTS, SUSE Linux
Enterprise (SLES) 12. While OpenShift supports only RHEL 7.4 or later in 3.x, or
Red Hat Enterprise Linux CoreOS (RHCOS) in release 4.x. In OpenShift Container
Platform 4.1, you must use RHCOS for all masters, but you can use Red Hat
Enterprise Linux (RHEL) as the operating system for compute, or worker,
machines. If you choose to use RHEL workers, you must perform more system
maintenance than if you use RHCOS for all of the cluster machines.</p>
</div>
<div class="paragraph">
<p>What does this mean is that you need to switch RHEL or RHCOS when migrating ICP
running on Ubuntu or Suse Linux. Most of this is infrastructure related Ops
activity.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_storage"><a class="anchor" href="#_storage"></a>Storage</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_security"><a class="anchor" href="#_security"></a>Security</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_selinux"><a class="anchor" href="#_selinux"></a>SELinux</h3>
<div class="paragraph">
<p>OpenShift requires SELinux to be <code>enforcing'' and </code>targeted'' mode. When
containers are run, the container image’s filesystem is labeled using a random
label and the container processes are labeled the same way, so that only the
container processes can access its own filesystem and no other processes. Any
mounted filesystems (secrets, configmaps, or volumes) will have an SELinux
policy applied to them to allow the container to read and write to them.</p>
</div>
</div>
<div class="sect2">
<h3 id="_podsecuritypolicy_vs_securitycontextconstraints"><a class="anchor" href="#_podsecuritypolicy_vs_securitycontextconstraints"></a>PodSecurityPolicy vs SecurityContextConstraints</h3>
<div class="paragraph">
<p>OpenShift SecurityContextContsraints (SCC) is the pre-cursor to the
PodSecurityPolicy (PSP) in upstream community Kubernetes. As such, a lot of the
properties of the PSP come directly from the SCC. These objects are
cluster-scoped policies designed to limit the access of containers to the host
kernel. Most containers do not need to privileged access to the host and should
as a best practice not depend on the uid of the user owning the container
process. However, many containers on DockerHub and even some IBM middleware
require running as root or some other capabilities in order to function.</p>
</div>
<div class="paragraph">
<p>One important thing to note is that while the PodSecurityPolicy objects can be
created in OpenShift, the platform will ignore these objects and only enforces
the SecurityContextConstraints objects. OpenShift ships with some out of the box
SCCs, the default <code>restricted'' policy is the most restrictive, and the
</code>privileged'' policy is the most open.</p>
</div>
<div class="paragraph">
<p>One very large difference is that the default policy in OpenShift will generate
random a uid/gid from a range for the container process to run as (the
<code>restricted'' policy), and if your container depends on a specific uid/gid
being set, the container may not run. One common example is if container
requires reads or writes to the local filesystem as a specific user. In this
case, the </code>nonroot'' SCC seems to match the ``ibm-restricted-psp'' default
policy that ICP ships with.</p>
</div>
<div class="paragraph">
<p>Here is a comparison of the out-of-box SCCs to those shipped with ICP, as well
as some brief comments:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 34%;">
<col style="width: 33%;">
<col style="width: 33%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>OpenShift</strong></th>
<th class="tableblock halign-left valign-top"><strong>ICP</strong></th>
<th class="tableblock halign-left valign-top"><strong>Comments</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">anyuid</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ibm-anyuid-psp</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container is allowed to run as any uid, including root,
but within restricted SELinux context</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">hostaccess</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">(n/a)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container is allowed to access host namespaces (i.e. can
mount filesystem and network of the host), but must run as random non-root user</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">(n/a)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ibm-anyuid-hostaccess-psp</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container is allowed to access host
namespaces (i.e. can mount filesystem, access host network, and access any other
namespaced resources on the host), and may run as any user</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">hostmount-anyuid</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ibm-anyuid-hostpath-psp</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container is allowed to run as any
user and can mount host directories</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">hostnetwork</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">(n/a)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container can run on the host network, but must run as
random selected non-root user</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">nonroot</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ibm-restricted-psp</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Container can run as any user except root; this is
useful for containers that expect to run as a particular UID from its local
/etc/passwd</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">privileged</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ibm-privileged-psp</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Run as any user and have access to any host
features. This is essentially running as root right on the worker node and
should be used sparingly</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">restricted</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">(n/a)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">(OpenShift Default) Denies access to most host features and
must run as random-selected uid.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>In order for a pod to be able to run with additional access to the host system,
it’s necessary to apply the SCC to the service account the pod executes as. One
subtle difference between SCC and PSP is the RBAC around it; SCCs have a
<code>users'' property that lists the entities allowed to use the SCC while PSPs are
controlled with roles and rolebindings. You can use the following command to
apply the SCC to a service account, which under the covers adds the service
accounts to the </code>users'' property of the SCC.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc adm policy add-scc-to-user &lt;scc&gt; system:serviceaccount:&lt;namespace&gt;:&lt;serviceaccount&gt;
oc adm policy remove-scc-from-user &lt;scc&gt; system:serviceaccount:&lt;namespace&gt;:&lt;serviceaccount&gt;</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_identity_providers"><a class="anchor" href="#_identity_providers"></a>Identity Providers</h3>
<div class="paragraph">
<p>OpenShift supports one or more Identity Providers as user directory sources for
authentication. As OpenShift is a development platform, the default behavior is
that any user that can authenticate to OpenShift is able to create a project
(mappingMethod <code>claim''). This behavior can be changed during installation or
after installation by using mappingMethod </code>lookup'', the downside is that the
administrator must manually add user resources to OpenShift before they will be
authorized to use the platform.
<a href="https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#LookupMappingMethod">https://docs.OpenShift.com/container-platform/3.11/install_config/configuring_authentication.html#LookupMappingMethod</a>
for more information.</p>
</div>
</div>
<div class="sect2">
<h3 id="_role_based_access_control"><a class="anchor" href="#_role_based_access_control"></a>Role-based Access Control</h3>
<div class="paragraph">
<p>As Kubernetes RBAC was submitted upstream by Red Hat from OpenShift features,
much of the RBAC in ICP is largely the same in ICP and OpenShift. Roles and
ClusterRoles are groups of permissions on objects in the Kubernetes API.
RoleBindings and ClusterRoleBindings are objects that bind roles to identities
to access those permissions. Users, groups, and service accounts may have
multiple role bindings which aggregated together gives them an access list of
parts of the platform they may access.</p>
</div>
<div class="paragraph">
<p>One shortcut around assigning roles/cluster roles to users exists in the oc CLI,
which under the covers creates a RoleBinding or ClusterRoleBinding, instead of
the awkward <code>kubectl create rolebinding'' and </code>kubectl create
clusterrolebinding'' commands:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc adm policy add-role-to-user &lt;role&gt; &lt;user&gt;
oc adm policy add-cluster-role-to-user &lt;role&gt; &lt;user&gt;
oc adm policy remove-role-from-user &lt;role&gt; &lt;user&gt;
oc adm policy remove-cluster-role-from-user &lt;role&gt; &lt;user&gt;</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="_imagepolicy"><a class="anchor" href="#_imagepolicy"></a>ImagePolicy</h4>
<div class="paragraph">
<p>OpenShift also contains an image policy, although it is not stored as a Custom
Resource as it is in ICP. This can be configured on the master nodes. See:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/admin_guide/image_policy.html">https://docs.OpenShift.com/container-platform/3.11/admin_guide/image_policy.html</a></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_networking"><a class="anchor" href="#_networking"></a>Networking</h2>
<div class="sectionbody">
<div class="paragraph">
<p>From a developer point of view, the pod networking in OpenShift uses largely the
same concepts as ICP and Kubernetes in general. There are some implementation
differences in OpenShift networking to watch out for if you are managing the
platform.</p>
</div>
<div class="sect2">
<h3 id="_openshift_sdn"><a class="anchor" href="#_openshift_sdn"></a>OpenShift SDN</h3>
<div class="paragraph">
<p>The default networking implementation in OpenShift is the OpenShift SDN.</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html">https://docs.OpenShift.com/container-platform/3.11/architecture/networking/sdn.html</a></p>
</div>
<div class="paragraph">
<p>OpenShift SDN has with three different plugins that provide different levels of
network isolation between projects:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>ovs-subnet</strong>: (default) flat network that allows all projects to talk to all
projects</p>
</li>
<li>
<p><strong>ovs-multitenant</strong>: all projects are isolated from each other, with a single
exception the <code>default</code> project where the OpenShift router and internal image
registry run</p>
</li>
<li>
<p><strong>ovs-networkpolicy</strong>: allows fine-grained control of network isolation using
NetworkPolicy objects (equivalent to ICP).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When installing OpenShift, Red Hat recommends always installing using the
<strong>ovs-networkpolicy</strong> plugin which provides near parity with ICP feature with
Calico. To use this, add the following parameter to the ansible hosts file
before installation:</p>
</div>
<div class="paragraph">
<p><code>os_sdn_network_plugin_name='redhat/OpenShift-ovs-multitenant'</code></p>
</div>
<div class="paragraph">
<p>Note that it’s possible to run Calico on OpenShift instead of Openshfit SDN;
however Red Hat does not support this directly and the client will need to
purchase support directly from Tigera. The list of additional vendor-supported
network plugins are available here:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/install_config/configuring_sdn.html#admin-guide-configuring-sdn-available-sdn-providers">https://docs.OpenShift.com/container-platform/3.11/install_config/configuring_sdn.html#admin-guide-configuring-sdn-available-sdn-providers</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_openshift_sdn_architecture"><a class="anchor" href="#_openshift_sdn_architecture"></a>OpenShift SDN Architecture</h3>
<div class="paragraph">
<p>OpenShift SDN networking components live in the <code>openshift-sdn</code> project in
OpenShift, and consist of two daemonsets, <code>ovs</code> and <code>sdn</code>.</p>
</div>
<div class="paragraph">
<p><code>ovs</code> is a containerized version of Open vSwitch which is an open source SDN
software used most commonly in OpenStack. This will manage a bridge device,
vxlan tunnel device for the pod network, and all of the virtual ethernet devices
(veths) for each pod as they are created and destroyed.</p>
</div>
<div class="paragraph">
<p><code>sdn</code> is a component used to program openvswitch by synchronizing routes to
the other worker nodes and any cluster IP services created in the cluster. The
routes are programmed as open vswitch flows and the cluster IPs are configured
using netfilter (iptables) rules.</p>
</div>
<div class="paragraph">
<p>To dump the flows for debugging or informational purposes, you may install the
<code>`openvswitch'' package on any cluster node, and use `+ovs-ofctl+</code> to view the
flow table. See
<a href="https://docs.openshift.com/enterprise/3.1/admin_guide/sdn_troubleshooting.html#debugging-local-networking">https://docs.OpenShift.com/enterprise/3.1/admin_guide/sdn_troubleshooting.html#debugging-local-networking</a>
for more information. This output is helpful to understand how pod traffic is
forwarded.</p>
</div>
<div class="paragraph">
<p>In contrast to ICP/Calico, which uses a single controller pod running on the
master nodes to orchestrate subnet selection, routes and network policy rules,
and a daemonset <code>`calico-node'' running across each cluster node to program
iptables rules and do route propagation. In ICP/Calico, the `+kube-proxy+</code>
container running on every node programs the cluster IPs in iptables rules
instead of the <code>calico-node</code> pod.</p>
</div>
<div class="paragraph">
<p>In both ICP and Calico cases, the daemonset runs as a privileged container on
each host in order to have access to the host network.</p>
</div>
</div>
<div class="sect2">
<h3 id="_ip_address_management"><a class="anchor" href="#_ip_address_management"></a>IP Address Management</h3>
<div class="paragraph">
<p>As in standard Kubernetes, both OpenShift and ICP have a pod overlay network
where address space is defined for pods, and pod IP addresses are drawn from
subnets selected from this address space. In ICP this was defined using the
<code>`network_cidr'' property in the installation config.yaml. OpenShift also has
the same concept, where the cluster network CIDR defined in
`+osm_cluster_network_cidr+</code> in the ansible hosts file, the default is
<code>10.128.0.0/14</code>. You can view the subnet in the <code>clusternetwork</code> custom
resource in OpenShift (<code>oc get clusternetwork</code>).</p>
</div>
<div class="paragraph">
<p>Every node in the cluster will receive a <code>slice'' of this address space. One
additional parameter in OpenShift is the <code>+osm_host_subnet_length+</code>, which
defines the size of the subnets assigned to each node in the cluster where pods
running on them will be assigned IP addresses from. In ICP, Calico automatically
selected this size based on the number of nodes in the cluster and the size of
the pod network, and was able to resize and </code>steal'' subnets from other nodes
when particular worker nodes exhausted their pool. In OpenShift this is a static
length. The default value of this is 9, which indicates that every worker node
will get 32-9=23 bits of subnet space (i.e. a /23 subnet, or 512 IP addresses).
The assigned host subnets are stored in the <code>+hostsubnets+</code> Kubernetes custom
resource (<code>oc get hostsubnets</code>). It’s important to select a subnet length that
will satisfy both the number of worker nodes and the expected number of pods on
each worker node in the cluster.</p>
</div>
<div class="paragraph">
<p>Like in ICP, there is an additional <code>`service network'' overlay network, which
is a non-overlapping address space with the pod network that ClusterIP services
are defined on. In OpenShift the installation parameter for this is
`+openshift_portal_net+</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_pod_routing_and_route_propagation"><a class="anchor" href="#_pod_routing_and_route_propagation"></a>Pod Routing and Route Propagation</h3>
<div class="paragraph">
<p>In ICP, Calico propagated routes using a node-to-node mesh where every worker
node became a ``router'' for its assigned subnet on the pod network and the
routes were communicated using border gateway protocol (BGP). Since BGP is a
standard protocol used on the internet, it was possible for non-cluster nodes to
join the peer-to-peer mesh and the routes to be propagated outside of the
cluster and potentially gain some visibility into the pod network with external
tools. However, because of the node-to-node mesh there can be scalability issues
when the cluster becomes very large, BGP route reflectors could be used to
propagate routes instead.</p>
</div>
<div class="paragraph">
<p>In OpenShift, the routes are stored in Kubernetes resources and the ``sdn''
DaemonSet programs the routes on each cluster node as flows in the local
openvswitch tables. There is a bridge interface on each node that all pods
receive a port on, and a tunnel interface where all outbound pod network traffic
is sent when the destination pod is not running on the local node.</p>
</div>
<div class="paragraph">
<p>The following documentation helps to understand the network flows:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html#sdn-packet-flow">https://docs.OpenShift.com/container-platform/3.11/architecture/networking/sdn.html#sdn-packet-flow</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_network_isolation"><a class="anchor" href="#_network_isolation"></a>Network Isolation</h3>
<div class="paragraph">
<p>In contrast to ICP and Calico’s usage of iptables rules, OpenShift SDN uses
VXLAN to perform project-level isolation. Every project is assigned a Virtual
Network Identifier (VNID), and as traffic leaves the Open vSwitch tunnel, the
VNID is added to the outgoing packet. When traffic reaches the destination, if
the worker node does not have a policy (either the same VNID, or an explicit
Open vSwitch flow from a Network Policy) that allows the traffic, it is dropped.
As mentioned earlier the <code>default'' namespace runs the router and registry and
as such, every project is allowed to access this project, which is given the
special VNID 0. It’s important for administrators not to expose </code>default'' to
users to deploy pods in general as all projects in the cluster will have network
access to it.</p>
</div>
<div class="paragraph">
<p>You can read more details here:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html#network-isolation-multitenant">https://docs.OpenShift.com/container-platform/3.11/architecture/networking/sdn.html#network-isolation-multitenant</a></p>
</div>
<div class="paragraph">
<p>In some environments, OpenShift may run on top of infrastructure that already
uses VXLAN for isolation (such as VMware and NSX) and the VXLAN port used must
be changed due to conflicts. This can be done by following the steps documented
here:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/install_config/configuring_sdn.html#config-changing-vxlan-port-for-cluster-network">https://docs.OpenShift.com/container-platform/3.11/install_config/configuring_sdn.html#config-changing-vxlan-port-for-cluster-network</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_networkpolicy"><a class="anchor" href="#_networkpolicy"></a>NetworkPolicy</h3>
<div class="paragraph">
<p>NetworkPolicy is largely the same in OpenShift as it is in ICP. There is one
difference in that OpenShift only supports ingress NetworkPolicy, so network
policies with egress rules do not work and egress network policy is controlled
using a separate EgressNetworkPolicy object.</p>
</div>
<div class="paragraph">
<p>NetworkPolicy objects in OpenShift result in flow rules in Open vSwitch, and if
using a podSelector to match pods, the more pods that match the rule, the more
rules are created, which may cause some scalability issues. See documentation
for an explanation:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/admin_guide/managing_networking.html#admin-guide-networking-using-networkpolicy-efficiently">https://docs.OpenShift.com/container-platform/3.11/admin_guide/managing_networking.html#admin-guide-networking-using-networkpolicy-efficiently</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_egressnetworkpolicy_and_egressrouter"><a class="anchor" href="#_egressnetworkpolicy_and_egressrouter"></a>EgressNetworkPolicy and EgressRouter</h3>
<div class="paragraph">
<p>As mentioned in previous section, the OpenShift EgressNetworkPolicy is a
separate object used to control egress traffic from pods to external subnets.
These are implemented at Layer 3 in openflow table rules. The destinations may
also be DNS names, but these are implemented using a DNS lookup of the name and
the subsequent rules on the resolved IP address for the DNS record’s TTL. You
can see more information in the documentation here:
<a href="https://docs.openshift.com/container-platform/3.11/admin_guide/managing_networking.html#admin-guide-limit-pod-access-egress">https://docs.OpenShift.com/container-platform/3.11/admin_guide/managing_networking.html#admin-guide-limit-pod-access-egress</a></p>
</div>
<div class="paragraph">
<p>OpenShift has an object that allows all egress to a particular external service
go through a single node, called EgressRouter. This allows traffic coming from
the cluster to an external service appear from a static IP and allows operations
to whitelist that router. See:
<a href="https://docs.openshift.com/container-platform/3.11/admin_guide/managing_networking.html#admin-guide-limit-pod-access-egress-router">https://docs.OpenShift.com/container-platform/3.11/admin_guide/managing_networking.html#admin-guide-limit-pod-access-egress-router</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_dns"><a class="anchor" href="#_dns"></a>DNS</h3>
<div class="paragraph">
<p>ICP runs a DaemonSet across the masters containing CoreDNS for cluster DNS
lookup and name resolution. DNS was only available inside of pods, as the
kubelet would set each pod’s /etc/resolv.conf to point at the service IP address
of the CoreDNS pod, and the host’s /etc/resolv.conf is used for upstream name
resolution.</p>
</div>
<div class="paragraph">
<p>OpenShift 3.11 implements DNS slightly differently: SkyDNS runs on every node
and is embedded within the atomic-OpenShift-node service listening on port 53.
This node will sync service names and endpoints retrieved from etcd to the local
SkyDNS. Every node in the cluster will have its /etc/resolv.conf rewritten to
point at the local copy of SkyDNS. All pods will also have their
/etc/resolv.conf rewritten to point at the IP address of the local host. This
means that service names (using FQDN of the cluster internal domain) are
resolvable even from cluster nodes.</p>
</div>
<div class="paragraph">
<p>OpenShift will not start if NetworkManager is not enabled on all nodes. Make
sure that NetworkManager is managing all interfaces (NM_CONTROLLED=yes in
/etc/sysconfig/network-scripts/ifcfg-eth*). A script that runs when
NetworkManager brings up the interface will rewrite the local /etc/resolv.conf
to point at SkyDNS; the upstream DNS servers are stored in
/etc/origin/node/resolv.conf.</p>
</div>
<div class="paragraph">
<p>See the documentation for more information:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/architecture/networking/networking.html#architecture-additional-concepts-openshift-dns">https://docs.OpenShift.com/container-platform/3.11/architecture/networking/networking.html#architecture-additional-concepts-OpenShift-dns</a></p>
</div>
<div class="paragraph">
<p>Note that OpenShift 4.x implements this differently and has moved to the more
familiar CoreDNS.</p>
</div>
</div>
<div class="sect2">
<h3 id="_routes_vs_ingress"><a class="anchor" href="#_routes_vs_ingress"></a>Routes vs Ingress</h3>
<div class="paragraph">
<p>In order to get external cluster traffic into the cluster, ICP used the Proxy
Nodes which run an nginx-based ingress controller. Ingress resources stored in
Kubernetes were used to program the nginx configuration to accept Layer-7
traffic based on specific rules, and could leverage certain nginx features like
path-based rewrites and TLS termination using annotations on the ingress
resource.</p>
</div>
<div class="paragraph">
<p>In OpenShift, there is a similar component running on the <code>infra'' nodes called
the Router. This is an HAProxy container, and runs in the special </code>default''
project that all projects should have access to. OpenShift uses a special
<code>Route'' object that pre-dates </code>Ingress'' resources in Kubernetes, which can
be used to expose Layer 7 traffic, terminate TLS. There are a few more options
that are exposed as first-class properties of Routes such as being able to
passthrough TLS connections or re-encrypt them.</p>
</div>
<div class="paragraph">
<p>In later versions of OpenShift (3.10+), the router is able to translate
<code>Ingress'' objects to </code>Routes''. However, HAProxy is not as feature-rich as
nginx and as such some features in the ICP ingress controller are not available
using OpenShift routes, most notably path-based rewrites. A workaround is to run
a standalone nginx controller that can perform these rewrites as needed in each
project, and expose that using through the OpenShift router.</p>
</div>
<div class="paragraph">
<p>When OpenShift is installed, it requires a wildcard domain pointing at the IP
address or load balancer in front of the nodes where the router is installed
(<strong>OpenShift_hosted_registry_routehost</strong>). All routes will by default be given a
DNS name like &lt;route-name&gt;-&lt;project-name&gt;.&lt;app-subdomain&gt;.</p>
</div>
<div class="paragraph">
<p>More documentation about the default HAProxy router, including some advanced use
cases like router sharding (which is similar to the ICP isolated proxy use case)
is here:
<a href="https://docs.openshift.com/container-platform/3.11/install_config/router/default_haproxy_router.html#install-config-router-default-haproxy">https://docs.OpenShift.com/container-platform/3.11/install_config/router/default_haproxy_router.html#install-config-router-default-haproxy</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_external_integration_with_f5_load_balancer"><a class="anchor" href="#_external_integration_with_f5_load_balancer"></a>External Integration with F5 Load Balancer</h3>
<div class="paragraph">
<p>Note that like ICP, there is an F5 BIGIP controller for OpenShift where a
controller is able to program an F5 appliance through the API in response to
Kubernetes resources. See:
<a href="https://clouddocs.f5.com/containers/v2/openshift/">https://clouddocs.f5.com/containers/v2/OpenShift/</a></p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_operation_cluster_management_monitoring_and_logging"><a class="anchor" href="#_operation_cluster_management_monitoring_and_logging"></a>Operation – Cluster Management, Monitoring and Logging</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Operation maybe one of the complex areas requires extra planning and effort to
migrate from ICP to OpenShift.</p>
</div>
<div class="sect2">
<h3 id="_cluster_management"><a class="anchor" href="#_cluster_management"></a>Cluster Management</h3>
<div class="paragraph">
<p>We mentioned the different options to access ICP and OpenShift in early
chapters. From operation perspective either manual or automated, the command
line tools (cli) might be the most relevant tool. The good news is that both
platform support <code>kubectl'' to operate your cluster. The not so good news is
that both have their own flavor of cli (ICP has the cloudctl while OpenShift has
oc). Most of the standard kubernetes tasks can be carried out by sticking to
</code>kubectl''. That puts migration as small effort to migrate any <code>cloudctl''
command to either </code>kubectl'' or ``oc'' or sunset them.</p>
</div>
<div class="paragraph">
<p>One area you need to pay attention is that OpenShift runs only on RHEL or RHCOS
operating system. That may introduce some migration work when your ICP is
running on non-RedHat OS. For example, if you have operation scripts handles the
patches update on OS, service restart etc.</p>
</div>
</div>
<div class="sect2">
<h3 id="_monitoring"><a class="anchor" href="#_monitoring"></a>Monitoring</h3>
<div class="paragraph">
<p>Both platforms are adopting the CNCF projects as de-facto standard when comes to
monitoring. They are Grafana and Prometheus. ICP has fairly decent integration
with both technologies and OpenShift 3.11 installs them by default. But this
doesn’t mean the migration is that straightforward.</p>
</div>
<div class="paragraph">
<p>First, Prometheus may collect different set of metrics. It will be at least a
medium level of effort to adjust the Prometheus Query Language and tested in new
OpenShift platform.</p>
</div>
<div class="paragraph">
<p>Then, you might need to migrate the Grafana dashboards that purposely built for
ICP. OpenShift comes with some sample dashboard like Docker or Kubernetes
monitoring via Prometheus.</p>
</div>
<div class="paragraph">
<p>Alerting is another area you need to consider. In theory, OpenShift Prometheus
supports AlertManager (can be installed as optional component). But ensuring the
existing ICP alerts fully function in OpenShift including Notification by email,
webhooks, Slack, PagerDuty and alert Silencing, aggregation, inhibiting can take
quite bit of effort.</p>
</div>
</div>
<div class="sect2">
<h3 id="_logging"><a class="anchor" href="#_logging"></a>Logging</h3>
<div class="paragraph">
<p>ICP deploys an ELK (ElasticSearch, Logstash, Kibana) stack, referred to as the
management logging service, to collect and store all Docker-captured logs.</p>
</div>
<div class="paragraph">
<p>OpenShift uses the EFK (ElasticSearch, fluentd, Kibana) stack as a logging
solution. The main difference comparing to ICP is how the logs are shipped out
of the cluster with Fluentd. But most of that is implementation detail and
relatively transparent to the application and end user.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_migration_strategy_icp_cluster_migration"><a class="anchor" href="#_migration_strategy_icp_cluster_migration"></a>Migration Strategy – ICP Cluster migration</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="./migration_strategy.md">Migration Strategy</a>
== Migration Strategy – ICP Cluster migration
:toc:
:toc-placement!:</p>
</div>
<!-- toc disabled -->
<div class="sect2">
<h3 id="_openshift_installation"><a class="anchor" href="#_openshift_installation"></a>OpenShift Installation</h3>
<div class="paragraph">
<p>To migrate from ICP to Openshift, the recommended approach is a blue-green
deployment where an Openshift cluster is created alongside an existing ICP
cluster, the workload is migrated from ICP to Openshift, load balancers or DNS
entries are updated to point clients at the new cluster, and the ICP cluster is
retired.</p>
</div>
<div class="paragraph">
<p>We highly recommend infrastructure automation to create new clusters for
Openshift. This provides the quickest path for new cluster creation. We have
published infrastructure automation templates for ICP on various cloud
platforms, for example:</p>
</div>
<div class="paragraph">
<p><a href="https://github.com/ibm-cloud-architecture/terraform-icp-vmware" class="bare">https://github.com/ibm-cloud-architecture/terraform-icp-vmware</a></p>
</div>
<div class="paragraph">
<p>Terraform Examples for OpenShift will be published soon.</p>
</div>
<div class="paragraph">
<p>In scenarios where resources are limited, the approach involves draining and
removing under-utilized ICP worker nodes and using the capacity to build a small
Openshift control plane with a few workers. Depending on available capacity, an
Openshift cluster can start as a single master and scale up to three masters as
capacity is made available.</p>
</div>
</div>
<div class="sect2">
<h3 id="_user_migration"><a class="anchor" href="#_user_migration"></a>User Migration</h3>
<div class="sect3">
<h4 id="_user_authentication_migration_ldap"><a class="anchor" href="#_user_authentication_migration_ldap"></a>User Authentication migration – LDAP</h4>
<div class="paragraph">
<p>Kubernetes does not have users; it’s up to the Kubernetes distribution to
provide an authentication endpoint that performs user authentication and
identity mapping to either a User or a Group. Kubernetes does have roles and
cluster roles, which are used to group permissions, and rolebindings and
clusterrolebindings, which are used to assign permissions to particular users or
groups.</p>
</div>
<div class="paragraph">
<p>For more information about how authentication is implemented in Kubernetes, see:</p>
</div>
<div class="paragraph">
<p><a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/" class="bare">https://kubernetes.io/docs/reference/access-authn-authz/authentication/</a></p>
</div>
<div class="paragraph">
<p>In ICP, the internal auth-idp component is used as an OIDC provider that
authenticates users. This component can be configured from the UI and will
connect to LDAP on behalf of the cluster to authenticate users. The <code>Teams</code>
concept is used to group together users or groups from LDAP into logical groups
and is managed by the auth-idp component and persisted in mongodb.</p>
</div>
<div class="paragraph">
<p>Openshift 3.11 has a similar component embedded in the API server that performs
authentication on behalf of the cluster. However, it does not have a UI to
configure LDAP, so it takes some work during installation or after installation
to configure LDAP.</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#LDAPPasswordIdentityProvider" class="bare">https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#LDAPPasswordIdentityProvider</a></p>
</div>
<div class="paragraph">
<p>Openshift 4.x uses a separate operator to perform authentication, and another
operator manages that operator and its configuration using a
CustomResourceDefinition to configure LDAP and other identity providers.</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/4.1/authentication/identity_providers/configuring-ldap-identity-provider.html" class="bare">https://docs.openshift.com/container-platform/4.1/authentication/identity_providers/configuring-ldap-identity-provider.html</a></p>
</div>
<div class="paragraph">
<p>In our migration scenario, we specifically we looked at migrating an existing
ICP LDAP connection to Openshift 3.11.</p>
</div>
<div class="paragraph">
<p>Our LDAP server had the following contents:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>dn: dc=internal-network,dc=local
dc: internal-network
objectClass: top
objectClass: domain

dn: cn=ldapadm,dc=internal-network,dc=local
objectClass: organizationalRole
cn: ldapadm

dn: ou=People,dc=internal-network,dc=local
objectClass: organizationalUnit
ou: People

dn: ou=Group,dc=internal-network,dc=local
objectClass: organizationalUnit
ou: Group

dn: cn=binduser,dc=internal-network,dc=local
cn: binduser
objectClass: organizationalRole
objectClass: top
objectClass: simpleSecurityObject

dn: cn=user1,ou=People,dc=internal-network,dc=local
cn: user1
objectClass: person
objectClass: simpleSecurityObject
objectClass: uidObject
objectClass: top
sn: one
uid: user1

dn: cn=dev1,ou=Group,dc=internal-network,dc=local
cn: dev1
objectClass: groupOfUniqueNames
objectClass: top
uniqueMember: cn=user1,ou=People,dc=internal-network,dc=local
uniqueMember: cn=user2,ou=People,dc=internal-network,dc=local

dn: cn=user2,ou=People,dc=internal-network,dc=local
cn: user2
objectClass: person
objectClass: simpleSecurityObject
objectClass: uidObject
objectClass: top
sn: user two
uid: user2

dn: cn=clusteradmin,ou=People,dc=internal-network,dc=local
cn: clusteradmin
objectClass: person
objectClass: simpleSecurityObject
objectClass: uidObject
objectClass: top
sn: cluster admin
uid: clusteradmin

dn: cn=admin,ou=Group,dc=internal-network,dc=local
cn: admin
objectClass: groupOfUniqueNames
objectClass: top
uniqueMember: cn=clusteradmin,ou=People,dc=internal-network,dc=local

dn: cn=dev2,ou=Group,dc=internal-network,dc=local
cn: dev2
objectClass: groupOfUniqueNames
objectClass: top
uniqueMember: cn=user3,ou=People,dc=internal-network,dc=local
uniqueMember: cn=user4,ou=People,dc=internal-network,dc=local

dn: cn=user3,ou=People,dc=internal-network,dc=local
cn: user3
objectClass: person
objectClass: simpleSecurityObject
objectClass: uidObject
objectClass: top
sn: three
uid: user3

dn: cn=user4,ou=People,dc=internal-network,dc=local
cn: user4
objectClass: person
objectClass: simpleSecurityObject
objectClass: uidObject
objectClass: top
sn: four
uid: user4</pre>
</div>
</div>
<div class="paragraph">
<p>The ICP configuration appeared as follows:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/image1.png" alt="LDAP settings in ICP">
</div>
</div>
<div class="paragraph">
<p>The matching Openshift configuration was configured under identityProviders on
each master host in /etc/origin/master/master-config.yaml. Once this was
configured we restarted docker on each master host to restart the API server.
The configuration is as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>identityProviders:
  - name: "rhos-ldap"
    challenge: true
    login: true
    mappingMethod: claim
    provider:
      apiVersion: v1
      kind: LDAPPasswordIdentityProvider
      attributes:
        id:
        - dn
        email:
        - mail
        name:
        - cn
        preferredUsername:
        - uid
      bindDN: "cn=binduser,dc=internal-network,dc=local"
      bindPassword: "Letmein"
      insecure: true
      url: "ldap://192.168.100.4:389/dc=internal-network,dc=local?uid?sub?(objectclass=person)"</pre>
</div>
</div>
<div class="paragraph">
<p>Pay particular interest to the url. The format of the URL is</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ldap://host:port/basedn?attribute?scope?filter</pre>
</div>
</div>
<div class="paragraph">
<p>We have translated this from the ICP configuration, where <code>attribute</code> and
<code>filter</code> are built from the <code>User filter</code> in the ICP configuration. The
query it uses is:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>(&amp;(attribute=%v)(filter))</pre>
</div>
</div>
<div class="paragraph">
<p>Openshift has explicit <code>user</code> and <code>group</code> resources which the API server
manages. You can list them using the familiar <code>oc get users</code> and <code>oc get
groups</code> commands as well as create additional ones.</p>
</div>
<div class="paragraph">
<p>As Openshift is a developer platform, the default mappingMethod <code>claim</code> allows
anybody that successfully authenticates access to the platform to login and
create projects. When authentication is successful, the platform will create a
<code>user</code> resource automatically. The ICP model denies access to any users in
LDAP that are not part of a team. To match the ICP model and deny access to
anybody not explicitly added to Openshift there are two options:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>use the mappingMethod <code>lookup</code>. However this requires additional overhead as
the administrator must individually create users in the Openshift platform
before they are given access to log in to Openshift.</p>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#LookupMappingMethod" class="bare">https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#LookupMappingMethod</a></p>
</div>
<div class="paragraph">
<p>In our case, we created a user for user1, created an identity for it in ldap,
and then mapped them together:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ oc create user user1
user.user.openshift.io/user1 created

$ oc create identity rhos-ldap:cn=user1,ou=People,dc=internal-network,dc=local
identity.user.openshift.io/rhos-ldap:cn=user1,ou=People,dc=internal-network,dc=local created

$ oc create useridentitymapping rhos-ldap:cn=user1,ou=People,dc=internal-network,dc=local user1
useridentitymapping.user.openshift.io/rhos-ldap:cn=user1,ou=People,dc=internal-network,dc=local created</pre>
</div>
</div>
</li>
<li>
<p>Leave the default mappingMethod <code>claim</code> but deny access to create new
projects in Openshift. By default the <code>system:authenticated</code> group
(i.e. anybody in LDAP) is given the <code>self-provisioner</code> cluster-role, which
allows project creation. Removing the role removes the overhead of having to
create new users as they log in, but also prevents authenticated users from
consuming resources in the platform without cluster administrator action. See:
<a href="https://docs.openshift.com/container-platform/3.11/admin_guide/managing_projects.html#disabling-self-provisioning" class="bare">https://docs.openshift.com/container-platform/3.11/admin_guide/managing_projects.html#disabling-self-provisioning</a></p>
<div class="literalblock">
<div class="content">
<pre>$ oc patch clusterrolebinding.rbac self-provisioners -p '{ "metadata": { "annotations": { "rbac.authorization.kubernetes.io/autoupdate": "false" } } }'
$ oc patch clusterrolebinding.rbac self-provisioners -p '{"subjects": null}'</pre>
</div>
</div>
</li>
<li>
<p>We think this matches ICP the closest, but allowing users to create projects
on their own has some advantages in developer scenarios. Using the above policy
makes sense in production clusters but can be relaxed in development/test
clusters.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_group_migration_ldap"><a class="anchor" href="#_group_migration_ldap"></a>Group migration – LDAP</h4>
<div class="paragraph">
<p>Note that the Openshift api server does not query groups from LDAP; group
definitions must be synced manually. The documentation around this is here:
<a href="https://docs.openshift.com/container-platform/3.11/install_config/syncing_groups_with_ldap.html" class="bare">https://docs.openshift.com/container-platform/3.11/install_config/syncing_groups_with_ldap.html</a></p>
</div>
<div class="paragraph">
<p>In our scenario we had users in the tree under ou=People, and groups under
ou=Group. Three groups were created (dev1, dev2, and admins). We used the
following rfc2307 LDAP sync config:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>kind: LDAPSyncConfig
apiVersion: v1
url: ldap://192.168.100.4:389/dc=internal-network,dc=local
bindDN: "cn=binduser,dc=internal-network,dc=local"
bindPassword: "Letmein"
insecure: true
rfc2307:
    groupsQuery:
        baseDN: "ou=Group,dc=internal-network,dc=local"
        scope: sub
        derefAliases: never
        pageSize: 0
        filter: "(objectclass=groupOfUniqueNames)"
    groupUIDAttribute: dn
    groupNameAttributes: [ cn ]
    groupMembershipAttributes: [ uniqueMember ]
    usersQuery:
        baseDN: "ou=People,dc=internal-network,dc=local"
        scope: sub
        derefAliases: never
        pageSize: 0
    userUIDAttribute: dn
    userNameAttributes: [ uid ]
    tolerateMemberNotFoundErrors: false
    tolerateMemberOutOfScopeErrors: false</pre>
</div>
</div>
<div class="paragraph">
<p>Observe how this maps to the configuration in ICP; the groups are of object
class <code>groupOfUniqueNames</code> and the <code>uniqueMember</code> attribute contains the
members of the group which will be in turn queried.</p>
</div>
<div class="paragraph">
<p>Running this command will add some Openshift <code>Group</code> resources that can be
assigned roles.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc adm groups sync --sync-config=rfc2307_config.yaml  --confirm
group/dev1
group/admin
group/dev2</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is three groups, with the user mappings as shown.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ oc get groups
NAME      USERS
admin     clusteradmin
dev1      user1, user2
dev2      user3, user4</pre>
</div>
</div>
<div class="paragraph">
<p>As this is a manual process that produces static user/group mappings, it may be
required to run this on a schedule that updates and prunes groups in an ongoing
basis.</p>
</div>
<div class="paragraph">
<p>One additional implementation note is that Openshift issues Opaque tokens; since
the authentication module is embedded in the API server it is able to validate
the tokens internally. In ICP, the authentication token issued by the auth
service is a signed JWT that contains an embedded list of groups that Kubernetes
uses to validate permissions. In the next session when we discuss RBAC, we can
see how rolebindings and clusterrolebindings are bound to these groups.</p>
</div>
</div>
<div class="sect3">
<h4 id="_user_authorization_migration_rbac"><a class="anchor" href="#_user_authorization_migration_rbac"></a>User Authorization Migration (RBAC)</h4>
<div class="paragraph">
<p>Kubernetes ships with some out of box user-facing cluster roles:
<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles" class="bare">https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles</a></p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>cluster-admin</code></p>
</li>
<li>
<p><code>admin</code></p>
</li>
<li>
<p><code>edit</code></p>
</li>
<li>
<p><code>view</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>ICP ships with five additional user-facing cluster roles which correspond to the
roles used in teams:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>icp:admin</code></p>
</li>
<li>
<p><code>icp:edit</code></p>
</li>
<li>
<p><code>icp:operate</code></p>
</li>
<li>
<p><code>icp:teamadmin</code></p>
</li>
<li>
<p><code>icp:view</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>These are implemented as aggregated roles, which means there are several roles
where their permissions are union-ed together to provide a full set of
permissions for the user. The permissions are contained in aggregate roles. Note
that the admin role contains all of the permissions that the edit role contains,
etc.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>icp-admin-aggregate</code></p>
</li>
<li>
<p><code>icp-edit-aggregate</code></p>
</li>
<li>
<p><code>icp-operate-aggregate</code></p>
</li>
<li>
<p><code>icp-view-aggregate</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Openshift ships with four user-facing cluster roles:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>admin</code></p>
</li>
<li>
<p><code>basic-user</code></p>
</li>
<li>
<p><code>edit</code></p>
</li>
<li>
<p><code>view</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Since RBAC is common in both Openshift and ICP, it’s tempting to just export
these 9 roles to Openshift. However, some Openshift-specific resources are added
to the Kubernetes out-of-box roles. These are in the <code>*.openshift.io</code>
apiGroups defined in the roles for the above.</p>
</div>
<div class="paragraph">
<p>As clusterroles are whitelists of permissions, and access in Kubernetes is a
union of all of the roles bound to the identity, one potential way of quickly
migrating permissions is to import the cluster roles from ICP, assign the ICP
roles to the users and groups, then assign the Openshift <code>view</code> role to all
users so that the projects will appear in the CLI and UI. The Openshift <code>view</code>
role will overlap with the <code>icp:view</code> role but the Openshift <code>view</code> role
will enable view of the Openshift specific resources (e.g. projects, builds,
etc). This will allow the same access to the Kubernetes API that was assigned in
ICP. Additional access to Openshift specific objects (e.g. to create a build,
deployment configs, etc) may be added by assigning Openshift specific roles like
<code>edit</code>, <code>admin</code>, etc.</p>
</div>
<div class="paragraph">
<p>Here is one team that we created with the following group/user mappings:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ cloudctl iam team-get dev-1
Name: dev1
ID: dev-1

ID      Type    Name    Email   Roles
dev1    group   -       -       Viewer
user1   user    user1   -       Administrator</pre>
</div>
</div>
<div class="paragraph">
<p>With the following resources assigned:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ cloudctl iam resources -t dev-1
CRN
crn:v1:icp:private:k8:jkwong-icp-31-cluster:n/dev1:::</pre>
</div>
</div>
<div class="paragraph">
<p>We exported the 9 ICP roles from ICP as yamls, and imported them into openshift.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ kubectl get clusterroles | grep icp | awk '{print $1}' | xargs -n1 -I{} kubectl get clusterrole {} -o yaml  | tee  all-roles.yaml

...

$ oc apply -f all-roles.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>Then we created the associated resources (i.e. the namespace/project) and mapped
the same permissions using the following commands:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ oc new-project dev1
Now using project "dev1" on server "https://console.jkwong-ocp.internal-network.local:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-25-centos7~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby.</pre>
</div>
</div>
<div class="paragraph">
<p>We initially gave access to both of these entities:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ oc adm policy add-role-to-group view dev1
role "view" added: "dev1"
$ oc adm policy add-role-to-user view user1
role "view" added: "user1"</pre>
</div>
</div>
<div class="paragraph">
<p>Next we assigned the ICP role to give the same access that the user had in ICP.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ oc adm policy add-role-to-group icp:view dev1
role "icp:view" added: "dev1"
$ oc adm policy add-role-to-user icp:admin user1
role "icp:admin" added: "user1"</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_workload_migration"><a class="anchor" href="#_workload_migration"></a>Workload Migration</h3>
<div class="paragraph">
<p>In this section we used a case study of migrating our cloud native reference
application BlueCompute that was running on ICP to Openshift.</p>
</div>
<div class="paragraph">
<p><a href="https://github.com/ibm-cloud-architecture/refarch-cloudnative-kubernetes" class="bare">https://github.com/ibm-cloud-architecture/refarch-cloudnative-kubernetes</a></p>
</div>
<div class="paragraph">
<p>For detailed steps on how to migrate Bluecompute from ICP to Openshift, refer this doc - <a href="https://github.com/ibm-cloud-architecture/refarch-cloudnative-kubernetes/blob/spring/docs/openshift/README.md">BlueCompute on OpenShift</a>.</p>
</div>
<div class="sect3">
<h4 id="_modify_containers_to_run_as_non_root_and_other_mitigations"><a class="anchor" href="#_modify_containers_to_run_as_non_root_and_other_mitigations"></a>Modify containers to run as non-root and other mitigations</h4>
<div class="paragraph">
<p>Following some guidelines here:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/creating_images/guidelines.html" class="bare">https://docs.openshift.com/container-platform/3.11/creating_images/guidelines.html</a></p>
</div>
<div class="paragraph">
<p>Openshift specific:</p>
</div>
<div class="paragraph">
<p><a href="https://docs.openshift.com/container-platform/3.11/creating_images/guidelines.html#openshift-specific-guidelines" class="bare">https://docs.openshift.com/container-platform/3.11/creating_images/guidelines.html#openshift-specific-guidelines</a></p>
</div>
<div class="paragraph">
<p>In general, when authoring containers, developers should try run with the least
privileges as possible.</p>
</div>
<div class="sect4">
<h5 id="_modifying_a_containers_user"><a class="anchor" href="#_modifying_a_containers_user"></a>Modifying a container’s USER</h5>
<div class="paragraph">
<p>If a container’s Dockerfile does not set a USER, then it runs as root by
default.This is dangerous because root inside a container is also root on the
host. Openshift prevents containers from running as <code>root</code> by applying a
default <code>restricted</code> SecurityContextConstraint. When a container is started,
Openshift will randomly select a uid from a range that does not have access to
anything on the worker node in case a malicious container process is able to
break out of its sandbox.</p>
</div>
<div class="paragraph">
<p>In most application scenarios, the actual user a process runs as doesn’t matter,
but there are some legitimate cases where the container expects to be run as a
particular user, such as some database containers or other applications where it
needs to read or write to its local filesystem or to a persistent volume. A
simple mitigation is to add the <code>USER</code> directive to the Dockerfile before the <code>CMD</code>
or <code>ENTRYPOINT</code> so that the main container process does not run as root, e.g.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>USER 1000</pre>
</div>
</div>
<div class="paragraph">
<p>Then making sure the files it modifies contains the correct permissions.</p>
</div>
<div class="paragraph">
<p>It’s better to provide a numeric value rather than an existing user in
/etc/passwd in the container’s filesystem, as Openshift will be able to validate
the numeric value against any SCCs that restrict the uids that a container may
run as. In the case where we use a third party container and we are not able to
modify the Dockerfile, or the <code>USER</code> directive refers to a user that corresponds
to something in /etc/passwd, we can add the securityContext section to the
podspec to identify the UID that it the pod refers to. For example, in
BlueCompute the MySQL container we used is from dockerhub, but they allow
running as <code>USER</code> mysql which corresponds to uid <code>5984</code> in /etc/passwd, so we added
this section to the podSpec in the deployment:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>securityContext:
  runAsUser: 5984
  runAsGroup: 5984
  fsGroup: 1000</pre>
</div>
</div>
<div class="paragraph">
<p>The fsGroup is useful to provide supplemental groups which are added to the
container’s processes. For example, in the above case the container process can
also interact with files owned by group 1000, which might be helpful if using
existing shared storage where there are directories owned by the group.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_modifying_a_containers_filesystem_for_readwrite"><a class="anchor" href="#_modifying_a_containers_filesystem_for_readwrite"></a>Modifying a container’s filesystem for read/write</h4>
<div class="paragraph">
<p>If filesystem access is needed in the container filesystem, then those files
should be owned by and read/writable by the root group. In Openshift, the
arbitrary uid used by the <code>restricted</code> SCC will be added to the <code>root</code> group.
Directories that must be read to/written from as scratch space may add the
following to the Dockerfile:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>RUN chgrp -R 0 /some/directory &amp;&amp; \
    chmod -R g=u /some/directory</pre>
</div>
</div>
<div class="paragraph">
<p>Another strategy that we’ve had success with is to create an <code>emptyDir</code> volume and
mount it to the directory, which Kubernetes will create and destroy with the
pod. The emptyDir volume is owned by root but is world writable and can be used
as local storage for the container. This also helps someone reviewing the pod
definition identify which directories will be written to.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>volumes:
- emptyDir: {}
  name: database-storage</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_using_a_nonroot_securitycontextconstraint"><a class="anchor" href="#_using_a_nonroot_securitycontextconstraint"></a>Using a <code>nonroot</code> SecurityContextConstraint</h4>
<div class="paragraph">
<p>In cases where existing shared storage is attached to the container as a volume,
and the container must write to the filesystem as a particular uid, we can run
the pod under a service account, and assign the "nonroot" scc to the service
account. This relaxes the restrictions on the uid and allows the container
process to be executed as some specific UID, but not root. This is essentially
equivalent to ICP’s <code>ibm-restricted-psp</code> security policy. For example, in our
above example our MySQL container runs as uid 5984, but Openshift blocks the
execution since the uid is not within the allowed range for<code>restricted</code> SCC, so
we created a service account <code>inventory-mysql</code> for the pod, allow it to use
the <code>nonroot</code> SCC and set the deployment to run as this service account.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ oc create serviceaccount inventory-mysql
$ oc adm policy add-scc-to-user nonroot -z inventory-mysql
$ oc patch deploy/inventory-mysql --patch \
     '{"spec":{"template":{"spec":{"serviceAccountName":"inventory-myql"}}}}}'</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_using_anyuid_seucirtycontextconstraint"><a class="anchor" href="#_using_anyuid_seucirtycontextconstraint"></a>Using <code>anyuid</code> SeucirtyContextConstraint</h4>
<div class="paragraph">
<p>In cases where running as the root user is absolutely necessary, we can leverage
a serviceaccount and assign the <code>anyuid</code> SCC to allow the container to run as
root. For example, in our BlueCompute reference application, couchdb only works
when it runs as root due to the way it tries to change ownership of the data
directories. We will allow couchdb to run as root user:</p>
</div>
<div class="paragraph">
<p>Steps in ICP:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ kubectl ceate serviceaccount -n bluecompute customer-couchdb
$ kubectl create role psp-ibm-anyuid-psp -n bluecompute --verb=use \
     --resource=podsecuritypolicy --resource-name=ibm-anyuid-psp
$ kubectl create rolebinding couchdb-ibm-anyuid-psp -n bluecompute \
     --role=psp-ibm-anyuid-psp --serviceaccount=bluecompute:customer-couchdb</pre>
</div>
</div>
<div class="paragraph">
<p>Now we add the serviceaccount to the podspec in the statefulset.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ kubectl patch deploy/couchdb --patch \
     '{"spec":{"template":{"spec":{"serviceAccountName":"customer-couchdb"}}}}}'</pre>
</div>
</div>
<div class="paragraph">
<p>Similarly, in Openshift:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ oc create serviceaccount -n bluecompute customer-couchdb
$ oc adm policy add-scc-to-user anyuid -z customer-couchdb
$ oc patch deploy/couchdb --patch \
     '{"spec":{"template":{"spec":{"serviceAccountName":"customer-couchdb"}}}}}'</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_devops_and_developer_toolchains"><a class="anchor" href="#_devops_and_developer_toolchains"></a>DevOps and Developer toolchains</h3>
<div class="paragraph">
<p>Generally, ICP was not opinionated on DevOps, and if the toolchain is outside of
the platform there is no reason for this to change when migrating to Openshift.</p>
</div>
<div class="paragraph">
<p>Openshift’s value proposition does include some developer productivity tools
such as Source-to-Image (S2I) and containerized Jenkins, as well as tech preview
and support for Openshift Pipelines and CodeReady Workspaces, but in a migration
scenario where we are migrating existing workload from ICP these are not
required to change. We can simply treat Openshift as another Kubernetes platform
we are deploying to.</p>
</div>
<div class="sect3">
<h4 id="_jenkins_server_migration"><a class="anchor" href="#_jenkins_server_migration"></a>Jenkins server migration</h4>
<div class="paragraph">
<p>One scenario that bears mentioning is if the CI toolchain was deployed to ICP,
typically using the community Helm chart. For example, In the BlueCompute case,
this was done using a containerized Jenkins instance. All of the stages in the
pipelines run as containers in Kubernetes using the Kubernetes plugin.</p>
</div>
<div class="paragraph">
<p><a href="https://github.com/ibm-cloud-architecture/refarch-cloudnative-devops-kubernetes" class="bare">https://github.com/ibm-cloud-architecture/refarch-cloudnative-devops-kubernetes</a></p>
</div>
<div class="paragraph">
<p>The best practice around Jenkins is for the pipelines themselves to be written using a declarative <code>Jenkinsfile</code> stored with the code.
The pipeline logic is stored with the application source code and itself treated as part of the application.
In BlueCompute, Jenkinsfiles are stored with each microservice, so we only needed to create an instance of Jenkins in Openshift and import the pipelines to get our builds to work.</p>
</div>
<div class="paragraph">
<p>Openshift has both ephemeral and persistent Jenkins in the catalog which is very comparable to the Jenkins Helm chart.
This instance of Jenkins will automatically install the Kubernetes and Openshift client plugins, so Jenkinsfile that uses podTemplates that spin up containers to run stages in the pipeline should "just work".</p>
</div>
</div>
<div class="sect3">
<h4 id="_running_jenkins_pipeline_stages_as_containers_in_openshift"><a class="anchor" href="#_running_jenkins_pipeline_stages_as_containers_in_openshift"></a>Running Jenkins pipeline stages as containers in Openshift</h4>
<div class="paragraph">
<p>As pipeline stages are run in containers, there are security issues when a particular stage attempts to run a container that requires root access, mounts a hostpath, etc, just like application containers.
Most runtime build images don’t need to run as root (e.g. maven, gradle, etc), which means they should run just fine in Openshift.</p>
</div>
<div class="paragraph">
<p>One security problem when using Kubernetes plugin is how to build the container image itself, which must run in a container.
In Openshift this is further complicated that the default <code>restricted</code> SCC disallows host mounting the docker socket, running as root, or running in privileged mode without changing the SCC.
The community is still attempting to resolve this problem, since traditionally the Docker tool requires root and many of the functions involved in building a container image requires various elevated privileges.</p>
</div>
<div class="paragraph">
<p>There are a few projects at various stages in development as of this writing that can build container images without docker, which run fine outside of a container, but none are perfect inside of a container.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/GoogleContainerTools/kaniko">kaniko</a></p>
</li>
<li>
<p><a href="https://github.com/cyphar/orca-build">orca-build</a></p>
</li>
<li>
<p><a href="https://github.com/genuinetools/img">img</a></p>
</li>
<li>
<p><a href="https://github.com/uber/makisu">makisu</a></p>
</li>
<li>
<p><a href="https://github.com/containers/buildah">buildah</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When using these tools, we can relax the security constraints on them by adding the SCC to the <code>jenkins</code> service account in the namespace where the pod runs.
For example, kaniko only requires <code>root</code> access, so we can simply add the <code>anyuid</code>
scc to enable kaniko:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ oc adm policy add-scc-to-user anyuid -z jenkins</pre>
</div>
</div>
<div class="paragraph">
<p>Note the security implications of the above.
While containers are running with elevated privileges, any other workload running on the same worker node may be vulnerable, as Jenkins build stages may run as root.
Additionally since the developer controls the commands used in the pipeline stage, this essentially gives developers root access on the worker node the build runs on.
In shared clusters, it may make sense to use a <code>nodeSelector</code> on all projects where jenkins will run to isolate jenkins workloads to just a few nodes: <a href="https://docs.openshift.com/container-platform/3.11/admin_guide/managing_projects.html#setting-the-project-wide-node-selector" class="bare">https://docs.openshift.com/container-platform/3.11/admin_guide/managing_projects.html#setting-the-project-wide-node-selector</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_migration_of_container_builds_to_openshift_buildconfigs"><a class="anchor" href="#_migration_of_container_builds_to_openshift_buildconfigs"></a>Migration of Container builds to Openshift BuildConfigs</h4>
<div class="paragraph">
<p>If we are running the Jenkins pipeline stages in Jenkins on Openshift, we can leverage the <a href="https://github.com/openshift/jenkins-client-plugin">Openshift Jenkins client plugin</a> and the <a href="https://docs.openshift.com/container-platform/3.11/dev_guide/builds/index.html#defining-a-buildconfig">Openshift BuildConfig</a> to build the container image, which provides a controlled environment for producing the container image without exposing <code>root</code> access on any worker nodes.
By providing just a <code>Dockerfile</code> and a build context, Openshift will build and push the resulting image to the Openshift private registry and track it using an ImageStream, and we can then provide an
additional stage using <a href="https://github.com/containers/skopeo">skopeo</a> to push this to an external registry.
BuildConfig limits the amount of damage the developer can do because they are not explicitly running commands as root.</p>
</div>
<div class="paragraph">
<p>We have posted an example pipeline we used at a customer that leverages this at the following git repository:</p>
</div>
<div class="paragraph">
<p><a href="https://github.com/jkwong888/liberty-hello-world-openshift/blob/master/Jenkinsfile" class="bare">https://github.com/jkwong888/liberty-hello-world-openshift/blob/master/Jenkinsfile</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_continuous_deployment_and_gitops_adoption"><a class="anchor" href="#_continuous_deployment_and_gitops_adoption"></a>Continuous Deployment and GitOps adoption</h4>
<div class="sect4">
<h5 id="_gitops_migration"><a class="anchor" href="#_gitops_migration"></a>GitOps Migration</h5>
<div class="paragraph">
<p>In some of our previous CICD examples, we tightly coupled Continuous Integration (CI) with Continuous Deployment (CD) in a single pipeline.
This makes it difficult for deployments to become portable as the build is highly dependent on the deployment target.</p>
</div>
<div class="paragraph">
<p>The community has been moving toward declarative deployment and some terms like <a href="https://www.weave.works/technologies/gitops/">GitOps</a> have been gaining popularity.
In this model, CI performs pre-release verification with build and test of the application.
The output of CI is the "document" that describes what to deploy, which in Kubernetes includes the container image, and the associated resource yamls that can also be packaged into Helm charts or kustomize applcations.
We leave the "how" and "where" to deploy to CD.
In some organizations these are separate (sometimes siloed) organizations, so the "document" approach allows a better separation of concerns.</p>
</div>
<div class="paragraph">
<p>In true GitOps, all operations have a matching action in git (pull requests, merges, etc).
Deployment documents are committed to a git repo, which can trigger webhooks to begin the CD process.
Code promotion involves merges to git repos representing the desired state of upper environments.
The git commit log could be used for approval and deployment history but probably shouldn&#8217;t be used for audit (as git history can be manipulated).</p>
</div>
<div class="paragraph">
<p>We have posted a sample CI pipeline where we decoupled the CI from the CD. (link to be provided)</p>
</div>
<div class="paragraph">
<p>As there is no upgrade path from Openshift 3.x to Openshift 4.x, separating CI from CD and adopting GitOps will allow you to add a new Openshift 4.x environment as an additional deployment target with minimal effort.
Having a single repository representing a deployment to a target environment also allows us to scale out to additional target environments as application adoption requires it.</p>
</div>
</div>
<div class="sect4">
<h5 id="_decoupling_cd_from_ci"><a class="anchor" href="#_decoupling_cd_from_ci"></a>Decoupling CD from CI</h5>
<div class="paragraph">
<p>As CI is common to both ICP and Openshift platforms, when we move to a declarative deployment model, the CD is the only part that changes.
A first step toward GitOps and declarative deployments is to decouple CD from CI.</p>
</div>
<div class="paragraph">
<p>In our Bluecompute case study, we had tightly coupled CI/CD pipelines and broke it into two, CI, and CD (to dev).
The end of our CI process generates an image, and embeds the URL into Kubernetes yamls and pushes it to a second Git repository representing a deployment.
The CD pipeline is triggered from changes to the deployment git repository, which executes a deployment script (in jenkins) into the target environments.
Generally we can expect one target environment type (e.g. dev, qa, stage, prod)per git repository, and one pipeline per environment (e.g. prod-east, prod-west).
It&#8217;s important to be as declarative as possible.</p>
</div>
<div class="paragraph">
<p>There are a few projects, including <a href="https://razee.io">Razee</a> (included with Cloud Pak for Applications), that can monitor and perform deployments from git repositories.
Ideally, the deployment is performed from inside the target environment to reduce the number of credentials being stored in the deployment system.
When these projects mature, we can migrate to them.
For now, we used our existing Jenkins server to monitor and perform the deployment.</p>
</div>
</div>
<div class="sect4">
<h5 id="_using_a_service_account_from_outside_of_the_platform_to_perform_deployment"><a class="anchor" href="#_using_a_service_account_from_outside_of_the_platform_to_perform_deployment"></a>Using a service account from outside of the platform to perform deployment</h5>
<div class="paragraph">
<p>Openshift ships with a service account in each project <code>deployer</code>, and a  cluster role <code>system:deployer</code> which contains some permissions it uses internally to perform <code>DeploymentConfig</code> rollouts.
As we are not using <code>DeploymentConfig</code>, this was not enough for our CD process, since all of the permissions are against the built-in Openshift resources.</p>
</div>
<div class="paragraph">
<p>In some of our client engagements, the CD process performs a deployment using a service account that has limited privileges.
For our Bluecompute case study, we created a <code>jenkins</code> service account that performs the deployment in the target namespace.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>oc create serviceaccount jenkins -n bluecompute</pre>
</div>
</div>
<div class="paragraph">
<p>We created the following <code>deployer</code> clusterrole that allows a CD tool to do its job on generic Kubernetes resources:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    openshift.io/description: Grants the right to deploy within a project.  Used primarily
      with service accounts for automated deployments.
    openshift.io/reconcile-protect: "false"
  name: deployer
rules:
- apiGroups:
  - extensions
  attributeRestrictions: null
  resources:
  - daemonsets
  - deployments
  - deployments/rollback
  - deployments/scale
  - ingresses
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicationcontrollers/scale
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch</pre>
</div>
</div>
<div class="paragraph">
<p>Then we apply this new role and also the <code>view</code> role to the <code>jenkins</code> service account so it can see the project.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>oc adm policy add-role-to-user view system:serviceaccounts:bluecompute:jenkins
oc adm policy add-role-to-user deployer system:serviceaccounts:bluecompute:jenkins</pre>
</div>
</div>
<div class="paragraph">
<p>We can also generate the <code>kubeconfig.yaml</code> needed for our CD tool to connect to the cluster.
First, find the serviceaccount token:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>oc describe serviceaccount jenkins
Name:                jenkins
Namespace:           bluecompute
Labels:              &lt;none&gt;
Annotations:         &lt;none&gt;
Image pull secrets:  jenkins-dockercfg-gmhlq
Mountable secrets:   jenkins-token-r4rcs
                     jenkins-dockercfg-gmhlq
Tokens:              jenkins-token-b7lvw
                     jenkins-token-r4rcs
Events:              &lt;none&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Get the token from the secret, which looks like a JWT.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>oc describe secret jenkins-token-b7lvw
Name:         jenkins-token-b7lvw
Namespace:    bluecompute
Labels:       &lt;none&gt;
Annotations:  kubernetes.io/created-by=openshift.io/create-dockercfg-secrets
              kubernetes.io/service-account.name=jenkins
              kubernetes.io/service-account.uid=64aed8e2-c900-11e9-b106-005056a86156

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:          1070 bytes
namespace:       11 bytes
service-ca.crt:  2186 bytes
token:           eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJ...</pre>
</div>
</div>
<div class="paragraph">
<p>Create a new kubeconfig yaml and log in to openshift using the token:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>export KUBECONFIG=/tmp/jenkins-kubeconfig.yaml
oc login &lt;openshift URL&gt; --token=eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJ...</pre>
</div>
</div>
<div class="paragraph">
<p>The file <code>/tmp/jenkins-kubeconfig.yaml</code> now contains the configuration including embedded credentials for connecting to Openshift as the service account.
Use caution when distributing this file as the token does not expire.
In our Bluecompute case study, we stored this as a "Secret file" credential in Jenkins.
You can invoke the API, for example to patch a deployment, using:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>kubectl --kubeconfig=&lt;mykubeconfig&gt; apply -f deployment.yaml</pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_performing_deployments_from_inside_the_platform"><a class="anchor" href="#_performing_deployments_from_inside_the_platform"></a>Performing deployments from inside the platform</h5>
<div class="paragraph">
<p>As the number of environments increases, it can become difficult to manage credentials for each environments, particular if they are dynamically created using Terraform, Cluster API, or some other infrastructure automation.
One interesting model is to run a controller or operator from inside each cluster that is created and have it monitor the git repository to perform deployments.
<a href="https://razee.io">Razee</a> is one such project, but there are others, including <a href="https://argoproj.github.io/argo-cd/">ArgoCD</a>.
Another approach is to just run a simple <a href="https://tekton.dev">Tekton</a> pipeline that monitors git repository changes and executes the deployment as the service account from within the cluster.
This way the credentials do not need to be extracted from each cluster as it is created and managed individually.</p>
</div>
<div class="paragraph">
<p>We will publish an example of this (link to be provided).</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_migration_of_kubernetes_artifacts"><a class="anchor" href="#_migration_of_kubernetes_artifacts"></a>Migration of Kubernetes Artifacts</h4>
<div class="sect4">
<h5 id="_deploymentconfig_vs_deployment"><a class="anchor" href="#_deploymentconfig_vs_deployment"></a>DeploymentConfig vs Deployment</h5>
<div class="paragraph">
<p>Openshift has its own deployment resources called <code>DeploymentConfig</code> and <code>ReplicationController</code>.
These pre-date the <code>Deployment</code> concept in Kubernetes and provide some additional functionality such as the ability to trigger deployments from upstream image changes.
As Openshift supports these same Kubernetes-native objects, the least effort in a migration is to reuse the resources used to deploy to ICP.</p>
</div>
</div>
<div class="sect4">
<h5 id="_routes_vs_ingress_2"><a class="anchor" href="#_routes_vs_ingress_2"></a>Routes vs Ingress</h5>
<div class="paragraph">
<p>Openshift also has the concept of <code>Route</code> instead of <code>Ingress</code>, which predates the first-class <code>Ingress</code> object.
Openshift will automatically convert <code>Ingress</code> resources to a <code>Route</code> and exposes them through the router.
See here for more information:
<a href="https://blog.openshift.com/kubernetes-ingress-vs-openshift-route/" class="bare">https://blog.openshift.com/kubernetes-ingress-vs-openshift-route/</a></p>
</div>
<div class="paragraph">
<p>As the implementation of how ingress resources are exposed are dependent on the ingress controller type, there are some features that are not available in Openshift routes.
One popular example is path-rewrites supported by annotations, which were performed by the nginx ingress controller in ICP.
A mitigation to this is to add the nginx ingress controller between the router and services in Openshift that monitors a specific ingress class, and expose the nginx ingress controller through a route.</p>
</div>
</div>
<div class="sect4">
<h5 id="_helm_charts"><a class="anchor" href="#_helm_charts"></a>Helm Charts</h5>
<div class="paragraph">
<p>Some development organizations may have adopted use of Helm Charts as a packaging mechanism for Kubernetes resources.
As has been discussed frequently, Openshift does not natively support Helm chart based deployment.
This is because it is difficult to configure the server-side component (Tiller) in a secure manner.
Helm does have some advantages, such as a robust templating language, versioning, and package dependency management.
A lot of community software is already packaged as Helm charts, although many of them do not work out of the box due in Openshift to problems with <code>SecurityContextConstraints</code> (discussed earlier).</p>
</div>
<div class="paragraph">
<p>If Helm charts are already used for package distribution in your application, we can continue to package applications using them as part of the deployment artifacts.
The deployment artifact in this case becomes the Helm chart in addition to generating the deployment values (i.e. <code>values.yaml</code>).
The deployment procedure involves rendering the template locally into raw Kubernetes yamls (<code>helm template</code>), then deploying them using the <code>kubectl</code> or <code>oc</code> CLI into the target cluster.
We have documented the approach here:
<a href="https://github.com/ibm-cloud-architecture/refarch-cloudnative-kubernetes/blob/spring/docs/openshift/README.md" class="bare">https://github.com/ibm-cloud-architecture/refarch-cloudnative-kubernetes/blob/spring/docs/openshift/README.md</a></p>
</div>
<div class="paragraph">
<p>Helm v3, which is currently in beta, is also a possibility for client-side rendering, as tiller is embedded into the helm client and there is no server-side component.  See the recent announcement about the beta as well as the related documentation:
<a href="https://helm.sh/blog/helm-v3-beta/" class="bare">https://helm.sh/blog/helm-v3-beta/</a></p>
</div>
<div class="paragraph">
<p>In the near term, Cloud Paks are distributed as Helm charts and the ICP common services layer will include server side tiller.
However it is not recommended to depend on this for application deployment as Cloud Pak will be moving away from server-side Tiller and into an Operator model.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_using_a_backuprestore_approach_using_velero"><a class="anchor" href="#_using_a_backuprestore_approach_using_velero"></a>Using a Backup/Restore approach using Velero</h4>

</div>
<div class="sect3">
<h4 id="_converting_podsecuritypolicy_to_securitycontextconstraints"><a class="anchor" href="#_converting_podsecuritypolicy_to_securitycontextconstraints"></a>Converting PodSecurityPolicy to SecurityContextConstraints</h4>

</div>
<div class="sect3">
<h4 id="_networkpolicy_migration"><a class="anchor" href="#_networkpolicy_migration"></a>NetworkPolicy migration</h4>

</div>
<div class="sect3">
<h4 id="_application_loadbalancer_cutover"><a class="anchor" href="#_application_loadbalancer_cutover"></a>Application LoadBalancer cutover</h4>

</div>
</div>
<div class="sect2">
<h3 id="_storage_migration"><a class="anchor" href="#_storage_migration"></a>Storage Migration</h3>
<div class="sect3">
<h4 id="_storage_migration_2"><a class="anchor" href="#_storage_migration_2"></a>Storage Migration</h4>
<div class="paragraph">
<p>We will focus on Kubernetes Storage under the context of ICP to OCP migration.
For detail storage and Kubernetes usage, please reference the
<a href="https://ibm-cloud-architecture.github.io/kubernetes-storage-cookbook/">Kubernetes
Storage Cookbook</a>.</p>
</div>
<div class="paragraph">
<p>The migration has to take into consideration of both the Kubernetes Storage
Provider and Storage consumer (database or application).</p>
</div>
<div class="sect4">
<h5 id="_storage_provider"><a class="anchor" href="#_storage_provider"></a>Storage Provider</h5>
<div class="paragraph">
<p>In general, Kubernetes supports quite a few storage providers including
hostPath, NFS, Ceph, Gluster, vSphere, minio, Cloud-based storage (S3 etc.). And
these providers can be deployed either as a part of a Kubernetes cluster
(internal storage) or storage provided by an external service (external
storage). For the migration, we’ll focus on the internal storage or in-cluster
storage provider.</p>
</div>
<div class="paragraph">
<p>Following storage can be hosted on ICP cluster nodes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>GlusterFS</p>
</li>
<li>
<p>Ceph block storage by using Rook</p>
</li>
<li>
<p>Minio</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Red Hat OpenShift support both GluserFS and Ceph as in-cluster storage
providers. Haven’t heard the official support for Minio.</p>
</div>
<div class="paragraph">
<p>There is no migration path or tools available to migrate ICP storage nodes to
OpenShift. So, it boils down to handle the migration from the storage consumer’s
aspect.</p>
</div>
<div class="paragraph">
<p>If you are using external storage provider, as far as it is supported by
OpenShift (all do except Minio), you just need to migrate the storage consumer
and leave the external storage provider as-is.</p>
</div>
<div class="paragraph">
<p>If you are using internal storage provider, you need to setup the OpenShift
Storage nodes, either GlusterFS or Ceph, using the same/similar spec as in ICP
in terms of disk size, storage type, number of nodes. Then, proceed to storage
consumer migration.</p>
</div>
</div>
<div class="sect4">
<h5 id="_storage_consumer"><a class="anchor" href="#_storage_consumer"></a>Storage Consumer</h5>
<div class="paragraph">
<p>Each client might have different storage consumption pattern, we’ll try to
categorize them into the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Container applications requires persistent Storage</p>
</li>
<li>
<p>Kubernetes Statefulset application</p>
</li>
<li>
<p>Databases running on Kubernetes such as MongoDB, MySQL, Cloudant etc.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>We’ll assume that all these storage needs are implemented as Kubernetes
recommended Persistent Volume (PV) and Persistent Volume Claims (PVC).</p>
</div>
<div class="paragraph">
<p>When it comes to migration to OCP, it really becomes a storage backup and
restore discussion. Depends on the storage consumer type (database vs. custom
application), it can be done with:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Kubernetes PV backup and restore</p>
</li>
<li>
<p>Using Application/Database native backup-restore tools</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This guide will be focus on the first approach where you migrate kubernetes PV.</p>
</div>
<div class="paragraph">
<p>One common approach of backing up Kubernetes PV is the
<a href="https://github.com/heptio/velero">Velero project</a> from Heptio. The concept is
Velero will take your PV snapshots, stores it on object storage (like S3 or
Minio). Then, you can restore it to another Kubernetes cluster.</p>
</div>
<div class="paragraph">
<p>For detail on how the tool works in generic Kubernetes, please reference
<a href="https://blog.kubernauts.io/backup-and-restore-of-kubernetes-applications-using-heptios-velero-with-restic-and-rook-ceph-as-2e8df15b1487">this
blog post</a></p>
</div>
<div class="paragraph">
<p>Still, there are some limitations with Velero approach. For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>It does not support the migration of persistent volumes across cloud
providers.</p>
</li>
<li>
<p>Velero + Restic currently supports backing up to only S3 compatible object
storage.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_platform_data_migration"><a class="anchor" href="#_platform_data_migration"></a>Platform Data Migration</h3>
<div class="sect3">
<h4 id="_monitoring_data"><a class="anchor" href="#_monitoring_data"></a>Monitoring Data</h4>

</div>
<div class="sect3">
<h4 id="_historical_log_data"><a class="anchor" href="#_historical_log_data"></a>Historical Log Data</h4>

</div>
</div>
</div>
</div>
</article>
</main>
</div>
<footer class="footer">
  <p>This page was built using the Antora default UI.</p>
  <p>The source code for this UI is licensed under the terms of the MPL-2.0 license.</p>
</footer>
<script src="../../_/js/site.js"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
